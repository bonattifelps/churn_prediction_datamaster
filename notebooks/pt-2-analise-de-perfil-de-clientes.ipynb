{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Analise de Perfil de clientes\n\nAqui o objetivo é entender mais sobre o padrão comportamente dos nossos clientes através da analise supervisionada.\nPara tanto, utilizaremos duas técnicas diferentes: de centroides e de hierarquização, para centoide direcionamos ao  K-means e Clusterização hierarquica.\n\nSeguiremos um passo a passo que consiste em:\n\n\n1. Remoção de outliers e PCA\n\n2. Numero de K otimo - Elbow\n\n3. Aplicar a clusterização usando K-means\n\n4. Aplicar a clusterização hierarquica\n\n5. Comparar os resultados - Rand Index e coeifciente de silueta","metadata":{}},{"cell_type":"markdown","source":"# A.Remoção de outliers e Redução de Dimensionalidade","metadata":{}},{"cell_type":"code","source":"def out_data(df):\n    for col in df.columns:\n        if (((df[col].dtype)=='float64') | ((df[col].dtype)=='int64')):\n            percentiles = df[col].quantile([0.01,0.99]).values\n            df[col][df[col] <= percentiles[0]] = percentiles[0]\n            df[col][df[col] >= percentiles[1]] = percentiles[1]\n        else:\n            df[col]=df[col]\n    return df\n\nfinal_df=out_data(join_tables)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_1 = final_df[:10000]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# B. Número ideal de k\n\nAs principais metodologias de definição de K são Cotovelo (elbow) e silhueta. Para definir com extatidão qual seguir vamos fazer  um comparação entre elas, com seus prós e contras:\n#### Método do Cotovelo (Elbow)\n#### Descrição:\nO método do cotovelo consiste em calcular a soma das distâncias quadráticas dentro dos clusters (inertia ou WCSS - Within-Cluster Sum of Squares) para diferentes valores de \\( K \\). O valor ideal de \\( K \\) é identificado onde há uma diminuição acentuada na curva, formando um \"cotovelo\".\n\n#### Prós:\n- Simplicidade: Fácil de implementar e interpretar visualmente.\n- Intuitivo: A curva do cotovelo fornece uma maneira visualmente clara de selecionar \\( K \\).\n#### Contras:\n- Subjetivo: A identificação do \"cotovelo\" pode ser ambígua, especialmente em datasets onde a curva é suave.\n- Escalabilidade: Pode ser computacionalmente caro para grandes datasets, pois requer múltiplas execuções do algoritmo de clustering.\n\nA técnica de normalização escolhidade é o minimos máximos\n\n**Devido ao alto processamento computacional seguiremos com o metodo do cotovelo.**","metadata":{}},{"cell_type":"code","source":"# Identificar colunas numéricas e categóricas\nnum_cols = sample_1.drop(['msno', 'safra', 'churn_1','is_churn', 'plan_list_price','payment_plan_days','is_auto_renew','bd',\n 'registration_ate_hoje','transaction_date_ate_expire_transactions_m1','transaction_date_ate_expire',\n                          'payment_method_id','payment_plan_days',\n ], axis =1).select_dtypes(include=np.number).columns.tolist()\n\n# Identificar colunas numéricas e categóricas\ncat_cols = sample_1.drop(['msno', 'safra', 'churn_1','is_churn', 'plan_list_price','payment_plan_days','is_auto_renew','bd',\n 'registration_ate_hoje','transaction_date_ate_expire_transactions_m1','transaction_date_ate_expire',\n                          'payment_method_id','payment_plan_days',\n ], axis =1).select_dtypes(include=np.object).columns.tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Iniciando parametros do kmeans\nkmeans_kwargs = {\n\"init\": \"random\",\n\"n_init\": 10,\n\"random_state\": 1,\n}\n\n# Normalizar os dados numéricos\nscaler = MinMaxScaler()\nscaler = scaler.fit(sample_1[num_cols])\nsample_1[num_cols] = scaler.transform(sample_1[num_cols])\n#print(scaled)\n\n\n#create list to hold SSE values for each k\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(sample_1[num_cols])\n    sse.append(kmeans.inertia_)\n\n#visualize results\nplt.plot(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Number of Clusters nobs \")\nplt.ylabel(\"SSE\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# C. Redução de dimensionalidade (PCA)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\npca = PCA(2)\ndata_pca = pca.fit_transform(sample_1[num_cols])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.scatter(data_pca[:, 0], data_pca[:, 1],\n            c=sample_1.is_churn, edgecolor='none', alpha=0.5)\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.colorbar();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# D. Clusterização\nPara o agrupamento utilizamos duas técnicas: k-means e GMM","metadata":{}},{"cell_type":"code","source":"# Aplicando K-Means\nkmeans = KMeans(n_clusters=4, random_state=42)\nkmeans_labels = kmeans.fit_predict(data_pca)\nkmeans_silhouette = silhouette_score(data_pca, kmeans_labels)\n\n# salvando na base\nsample_1['kmeans_cluster'] = kmeans_labels\n\n\n# Métrica de qualidade\nprint(f'Coeficiente de Silhueta - KMeans: {kmeans_silhouette:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for inverse transformation\nsample_1[num_cols] = scaler.inverse_transform(sample_1[num_cols])\nsample_1.groupby('kmeans_cluster').describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Clusterização usando GMM\n\nPara garantir que os dados estejam mais próximos de uma distribuição normal, você pode aplicar uma transformação logarítmica ou a transformação Box-Cox (que é mais flexível) antes da normalização. A seguir, o código atualizado com a transformação Box-Cox (caso os dados sejam estritamente positivos) antes da normalização.\n\n### Explicações:\n1. **PowerTransformer (Box-Cox)**: Essa transformação aproxima a normalidade para variáveis positivas, ajustando distribuições assimétricas ou com caudas longas. Use `method='box-cox'` para a transformação Box-Cox.\n  - Nota: O Box-Cox requer que os dados sejam **estritamente positivos**.\n  - Caso você tenha variáveis que incluem valores zero ou negativos, pode usar `method='yeo-johnson'`, que também é uma transformação de potência, mas não tem essa restrição.\n2. **StandardScaler**: Normaliza as variáveis numéricas transformadas para média 0 e desvio padrão 1.\n3. **OneHotEncoder**: Codifica as variáveis categóricas em formato binário (dummy variables).\n### Resultados:\nEsse pipeline agora trata adequadamente os dados numéricos, aplicando uma transformação que aproxima uma distribuição normal e em seguida normaliza os dados para que o **GMM** funcione melhor, especialmente em situações onde as variáveis não são originalmente gaussianas.\nSe você tiver dúvidas sobre o uso do Box-Cox ou a necessidade de outro tipo de transformação, é só avisar!","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.mixture import GaussianMixture\n\n# Criar transformações\ntransformacoes = ColumnTransformer(transformers=[\n   ('num', Pipeline(steps=[\n       #('boxcox', PowerTransformer(method='box-cox')),  # Transformação Box-Cox para aproximar normalidade\n       ('scaler', StandardScaler())  # Normalização\n   ]), num_cols),\n   ('cat', OneHotEncoder(), cat_cols)\n])\n# Criar pipeline com pré-processamento e GMM\npipeline = Pipeline(steps=[\n   ('pre_processamento', transformacoes),\n   ('gmm', GaussianMixture(n_components=3, random_state=42))\n])\n# Ajustar modelo\npipeline.fit(sample_1)\n# Prever clusters\nclusters = pipeline['gmm'].predict(pipeline['pre_processamento'].transform(sample_1))\nprint(\"Clusters:\", clusters)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Conclusão:**\nApós a escolha de 4 grupos através da distãncia intra clusters de elbow,clusterizamos com o algoritmo kmeans e abordagem de distancia euclidiana. Os grupos apresentaram distinção entre quem deu churn ou não, no mes target e outros. Outro  insight relevante foi a distinção de audição entre os grupos, e uma relação observada entre churn e a propria audição dos grupos.","metadata":{}}]}
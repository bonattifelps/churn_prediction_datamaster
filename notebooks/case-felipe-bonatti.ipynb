{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7263499,"sourceType":"datasetVersion","datasetId":4209836}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Avaliação de Churn\nO objetivo principal deste estudo é avaliar a probabilidade de churn dos clientes de um serviço de streaming nos próximos três meses, com a intenção de implementar políticas e ações que evitem a concretização desse cenário.\n\nO problema está dividido em duas partes:\n\nPrevisão de Churn: Análise da probabilidade de evasão de clientes.\nAnálise Não Supervisionada: Estudo do perfil comportamental dos clientes.","metadata":{"_uuid":"59d419be-458d-47bb-89cd-73cd3d9a7a10","_cell_guid":"be49ce55-58f8-4e47-9ae7-5f9101d0fe7a","trusted":true,"collapsed":false,"id":"rxx8jpdp6pWb","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Importações necessárias\nimport pyarrow.parquet as pq\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport scipy.cluster.hierarchy as sch\n\n##Dask\nimport json\nfrom dask import bag as db\nfrom dask import dataframe as dd\n\n# Importações de modelos e métricas do Scikit-Learn\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import (mean_squared_error, confusion_matrix, precision_score, \n                             recall_score, f1_score, roc_curve, auc, accuracy_score, \n                             silhouette_score, adjusted_rand_score)\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\n\n# Algoritmos de machine learning\nfrom sklearn import ensemble, linear_model, neighbors, tree, naive_bayes, svm\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\n\n# Configuração do pandas para exibição de colunas\npd.set_option('display.max_columns', 500)\n\n# Supressão de warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Leitura dos dados\nmembers = pq.read_table('/kaggle/input/case-data-master-2024/members.parquet')\ntransactions = pq.read_table('/kaggle/input/case-data-master-2024/transactions.parquet')\nlogs = pq.read_table('/kaggle/input/case-data-master-2024/user_logs.parquet')\n\n# Conversão para DataFrames pandas\nmembers_pd = members.to_pandas()\ntransactions_pd = transactions.to_pandas()\nlogs_pd = logs.to_pandas()\n\n# Liberação de memória\ndel members\ndel transactions\ndel logs","metadata":{"_uuid":"1bcb8ed5-82d8-472b-ba91-99f86c6b50f5","_cell_guid":"63d68c5a-ac91-4bb3-a55a-ffdfae221fe7","trusted":true,"collapsed":false,"id":"VACGDhEN6pWh","execution":{"iopub.status.busy":"2024-11-03T13:50:53.557357Z","iopub.execute_input":"2024-11-03T13:50:53.557855Z","iopub.status.idle":"2024-11-03T13:52:48.281460Z","shell.execute_reply.started":"2024-11-03T13:50:53.557804Z","shell.execute_reply":"2024-11-03T13:52:48.279810Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Contexto\n   \nNeste estudo, daremos sequência à análise da evasão de clientes (churn) através de um modelo de classificação.\n\nA definição do alvo (target) é guiada por alguns aspectos-chave: o cliente está inativo? Cancelou seu plano? Não ouviu música nos últimos três meses?\n\nPara prever a saída de um cliente, é crucial observar a relação entre diferentes variáveis. Para isso, a avaliação de diversos cenários e hipóteses se torna necessária, algumas delas incluem:\n\nComportamento histórico do cliente: Qual foi o valor da sua assinatura e quanto música ele ouviu nos meses anteriores? Existe uma correlação com a possibilidade de churn?\nCaracterísticas socioeconômicas: Idade, gênero, cidade e canal de aquisição influenciam a taxa de churn?\nRelação entre a quantidade ouvida no mês anterior e o churn: Existe uma conexão significativa?\nDias desde o registro: Clientes mais novos apresentam maior propensão ao churn?\nAlém de testar essas hipóteses, é fundamental compreender como os dados se comportam e se relacionam com o nosso alvo, mantendo apenas as variáveis que são realmente importantes e que influenciam o problema em estudo.\n\nDessa forma, dividiremos nosso processo em dois capítulos, cada um contendo as seguintes subfases:\n\nModelo de Previsão de Churn\nAnálise superficial dos dados\nAnálise exploratória\nDefinição das variáveis para treinamento\nConstrução do alvo (target)\nSeleção de Features\nDivisão em conjuntos de treinamento e teste\nTreinamento do algoritmo\nHiperparametrização\nPrevisão do futuro e conclusão\nConclusão\nAnálise Não Supervisionada\nNormalização e PCA (Análise de Componentes Principais)\nAmostragem\nK-means\nConclusão","metadata":{"_uuid":"880fe3b4-f2ce-499f-a719-c42046bd375f","_cell_guid":"bc0d2477-b79a-4d9e-972f-a1e85db01d7a","trusted":true,"collapsed":false,"id":"sr1Ikmk36pWl","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"A. Análise Superficial dos Dados\nMembers\n\nCálculo do Tempo desde o Cadastro: Podemos desenvolver uma função que calcule o tempo decorrido desde o cadastro de cada cliente.\nVariáveis Categóricas:\nA variável \"cidade\" apresenta 21 entradas distintas.\nA variável \"gênero\" possui 3 categorias.\nA variável \"registered_via\" contém 17 categorias. Esses dados possibilitam a construção de um one-hot encoding para melhor representação das variáveis categóricas no modelo.\nDistribuição das Idades: A maior parte das idades dos clientes está entre 0 e 55 anos. O valor 0 provavelmente indica a falta de informação ou dados ausentes.\nTipos de Dados: Atualmente, todos os dados estão com o tipo dtype como object. Portanto, será necessário realizar um trabalho de conversão de tipos (casting) para adequar os dados aos tipos corretos.\nBase de Clientes Ativos: Na última safra disponível (201612), contabilizamos aproximadamente 1 milhão de clientes ativos, com um número similar na penúltima safra.\nUtilização da Base como Painel: Levaremos esta base como painel, pois as informações contidas são \"estáticas\" em relação ao cliente. Não é necessário manter o histórico dessas informações.","metadata":{"_uuid":"eb495816-9159-4df4-a431-d5e13f186c03","_cell_guid":"6c5e16c7-d2fc-4160-a721-3059d5e4c350","trusted":true,"collapsed":false,"id":"tjvJs6QM6pWn","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Visualização inicial das primeiras linhas e tipos de dados\nprint(members_pd.head())\nprint(members_pd.dtypes)","metadata":{"_uuid":"530868a8-5ae0-4b9e-a809-a858d45ecf53","_cell_guid":"4322d93e-9aad-47cc-83ea-54e232259853","trusted":true,"collapsed":false,"id":"KHhuPnTe6pWr","outputId":"47102cd5-219f-4478-d38d-2b7899c77701","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T13:54:08.167943Z","iopub.execute_input":"2024-11-03T13:54:08.169328Z","iopub.status.idle":"2024-11-03T13:54:08.185683Z","shell.execute_reply.started":"2024-11-03T13:54:08.169274Z","shell.execute_reply":"2024-11-03T13:54:08.184391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Exibição de contagem de valores únicos por coluna\nfor i in members_pd.columns:\n    print(i)\n    print(len(members_pd[i].unique()))\n    print(\"\")","metadata":{"_uuid":"663d71f6-9d75-4021-beca-261aa0ac108e","_cell_guid":"0d85f1af-357f-44c1-8f0f-88ad28e00a56","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T13:54:10.721667Z","iopub.execute_input":"2024-11-03T13:54:10.722155Z","iopub.status.idle":"2024-11-03T13:55:23.235850Z","shell.execute_reply.started":"2024-11-03T13:54:10.722108Z","shell.execute_reply":"2024-11-03T13:55:23.234504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convertendo a coluna 'bd' para numérico, forçando erros para NaN\nmembers_pd['bd'] = pd.to_numeric(members_pd['bd'], errors='coerce')\n\n# Removendo idades inválidas e substituindo-as por NaN\nmembers_pd['bd'] = members_pd['bd'].apply(lambda x: x if 0 < x < 100 else np.nan)\n\n# Transformação da coluna de registro de data para tipo datetime\nmembers_pd['registration_init_time'] = pd.to_datetime(members_pd['registration_init_time'], format='%Y%m%d')\n\n# Calculando o tempo desde o cadastro em dias\nlast_date = pd.to_datetime('2017-03-31')  # Última data conhecida nos dados\nmembers_pd['days_since_registration'] = (last_date - members_pd['registration_init_time']).dt.days\n\n# Visualizando as mudanças e o tratamento inicial de outliers\nprint(members_pd[['city', 'registered_via', 'bd', 'days_since_registration']].head())","metadata":{"_uuid":"7fac202f-dc4b-4d7d-bbd5-4f2c6fb359a0","_cell_guid":"293cf11b-d710-4729-bf20-06f9a6e847c3","trusted":true,"execution":{"iopub.status.busy":"2024-11-03T13:56:28.990080Z","iopub.execute_input":"2024-11-03T13:56:28.991370Z","iopub.status.idle":"2024-11-03T13:58:36.559389Z","shell.execute_reply.started":"2024-11-03T13:56:28.991319Z","shell.execute_reply":"2024-11-03T13:58:36.558114Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"###Transactions\n\nConstrução da Target: Precisaremos da tabela de transações para construir a variável alvo (target) do nosso modelo.\nConversão de Tipos (Casting): Assim como em outras tabelas, será necessário realizar a conversão de tipos (casting) para garantir que os dados estejam adequadamente formatados.\nDiferenciação de Clientes: É importante investigar a diferença entre clientes que pagam mensalmente e aqueles que optam pelo pagamento anual. Podemos modelar essa diferença, sendo mais prático dividir o preço pelo número de dias e normalizá-lo para um período padrão de 30 dias.\nVariáveis Booleanas e Categóricas:\nTemos algumas variáveis booleanas, como is_auto_renew e is_cancel, que indicam se a assinatura é renovada automaticamente e se foi cancelada, respectivamente.\nAlém disso, algumas variáveis podem ser tratadas como categóricas, como payment_method_id, que representa o método de pagamento utilizado.\nInatividade após Cancelamento: Se um cliente cancela a assinatura em um determinado mês, ele aparecerá como inativo no mês seguinte.\nUsuários Inativos: É importante notar que os usuários inativos não são registrados na base de pagamentos, o que pode influenciar nossas análises.\nTratamento de Dados Ausentes: Observamos poucos casos em que o período do plano é 0 na base de dados. Podemos utilizar uma técnica de preenchimento de dados ausentes (fillna) para lidar com essas situações.","metadata":{"_uuid":"76e40c14-5261-4f32-8bf7-ee6ae7ac0c9e","_cell_guid":"71e48cf3-a33d-46bc-848d-8310402dd141","trusted":true,"collapsed":false,"id":"gOdLRLL36pWs","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Visualizando as primeiras linhas e checando tipos de dados\ntransactions_pd.head()","metadata":{"_uuid":"0d40c8b5-a966-4929-868c-f6a2ae8f5b64","_cell_guid":"da333ae2-4c09-4955-90ce-f0e1f90d0d90","trusted":true,"collapsed":false,"id":"GedHBFZR6pWt","outputId":"3613399f-3d08-4774-c2a3-f1f5bd56ce8a","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T13:59:06.580223Z","iopub.execute_input":"2024-11-03T13:59:06.580686Z","iopub.status.idle":"2024-11-03T13:59:06.602167Z","shell.execute_reply.started":"2024-11-03T13:59:06.580645Z","shell.execute_reply":"2024-11-03T13:59:06.600831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Checando tipos de dados\ntransactions_pd.dtypes","metadata":{"_uuid":"88c25f76-36ad-4101-bedb-8b018a802f3e","_cell_guid":"c2a903e7-0be7-491a-b8b5-6c1344b25fe3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T13:59:09.800145Z","iopub.execute_input":"2024-11-03T13:59:09.800640Z","iopub.status.idle":"2024-11-03T13:59:09.810245Z","shell.execute_reply.started":"2024-11-03T13:59:09.800596Z","shell.execute_reply":"2024-11-03T13:59:09.809005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in transactions_pd.columns:\n    print(i)\n    print(len(transactions_pd[i].unique()))\n    print(\"\")","metadata":{"_uuid":"588b3a35-d9b9-4930-aaa0-9585cae3c604","_cell_guid":"090e2b5e-b1b5-467f-b181-803117d5792e","trusted":true,"collapsed":false,"id":"TGt_SX7_6pWv","outputId":"3da00189-b3b0-4d80-ea3b-0cb0c8277ba5","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T13:59:13.489879Z","iopub.execute_input":"2024-11-03T13:59:13.490348Z","iopub.status.idle":"2024-11-03T13:59:36.020946Z","shell.execute_reply.started":"2024-11-03T13:59:13.490306Z","shell.execute_reply":"2024-11-03T13:59:36.019497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Informação de Dias da transação até expiração da conta","metadata":{"_uuid":"a9a30cc0-c42c-47c6-840b-3628c9841ec5","_cell_guid":"fb002837-318a-491a-a83f-db9966049601","trusted":true,"collapsed":false,"id":"Vt0U7tbr6pWw","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Convertendo as colunas para o tipo datetime\ntransactions_pd.transaction_date = pd.to_datetime(transactions_pd.transaction_date, format='%Y%m%d', errors='coerce')\ntransactions_pd.membership_expire_date = pd.to_datetime(transactions_pd.membership_expire_date, format='%Y%m%d', errors='coerce')\n\n# Calculando os dias até a expiração da assinatura\ntransactions_pd['transaction_to_expire_days'] = (transactions_pd.membership_expire_date - transactions_pd.transaction_date).dt.days\n\n# Verificando as primeiras linhas após a adição da nova coluna\nprint(transactions_pd[['transaction_date', 'membership_expire_date', 'transaction_to_expire_days']].head())\n","metadata":{"_uuid":"331e1a72-23ba-4dab-8427-6496bb47998a","_cell_guid":"d4542fde-07cd-43b5-afd4-4c88f90ca264","trusted":true,"collapsed":false,"id":"mj0J2n5W6pWw","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T13:59:45.978379Z","iopub.execute_input":"2024-11-03T13:59:45.978912Z","iopub.status.idle":"2024-11-03T13:59:56.595680Z","shell.execute_reply.started":"2024-11-03T13:59:45.978864Z","shell.execute_reply":"2024-11-03T13:59:56.594574Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Log de audições","metadata":{"_uuid":"5b305441-f657-4726-b8c9-20027a6e7ce7","_cell_guid":"3323d303-a2be-46c4-abd4-09b4045cac3a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Simplicidade dos Dados: Os dados são em sua maioria simples e numéricos. A coluna total_secs representa a quantidade total de tempo que o cliente ouviu músicas durante o mês, sendo uma medida não cumulativa.\nValidação dos Dados: Não identificamos valores negativos nas variáveis, exceto na coluna total_secs, que necessitará de um tratamento específico para lidar com essas ocorrências atípicas.\nUso do Histórico: Dispomos de um histórico robusto de dados, permitindo a exploração de múltiplos meses para compreender a média histórica de consumo dos clientes. Isso possibilita a construção de variáveis preditivas (features) com base no comportamento passado. No entanto, devido a limitações de processamento, decidimos concentrar nossa análise em dados de apenas três meses.","metadata":{"_uuid":"6321023a-f568-430b-9de1-d98c4056cebe","_cell_guid":"9e07cfb6-ea79-4d3b-945b-818584488edf","trusted":true,"collapsed":false,"id":"ZXaT2Tyy6pWx","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Corrigindo valores negativos em 'total_secs' e mantendo valores positivos\nlogs_pd['total_secs'] = np.where(logs_pd['total_secs'] < 0, 0, logs_pd['total_secs'])\n\n# Visualizando as primeiras linhas após o ajuste\nprint(logs_pd.head())","metadata":{"_uuid":"7ded7d5c-1c1b-4255-881f-c98b43c47331","_cell_guid":"acb6bb89-9eea-463c-b544-623cfe850f1c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:01:05.783826Z","iopub.execute_input":"2024-11-03T14:01:05.784344Z","iopub.status.idle":"2024-11-03T14:01:06.014316Z","shell.execute_reply.started":"2024-11-03T14:01:05.784296Z","shell.execute_reply":"2024-11-03T14:01:06.013088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gerando estatísticas descritivas para validar as alterações\nprint(logs_pd.describe())","metadata":{"_uuid":"8e1f43f3-a969-4f8c-8273-b2e46bd57f05","_cell_guid":"7770907c-df7a-4290-ae86-4e822ca89517","trusted":true,"collapsed":false,"id":"NYdjVA_46pWx","outputId":"2c8c4225-aea0-4b91-eeba-32aaa31d3ace","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:01:09.308576Z","iopub.execute_input":"2024-11-03T14:01:09.309793Z","iopub.status.idle":"2024-11-03T14:01:21.085507Z","shell.execute_reply.started":"2024-11-03T14:01:09.309737Z","shell.execute_reply":"2024-11-03T14:01:21.084173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Lógica Temporal da Target e Features\n\nA estratégia dos executivos é prever a saída da carteira de clientes, o que implica em algumas considerações importantes:\n\nUso de Dados Atuais: Durante a implementação do modelo, utilizaremos dados recentes de cadastro e de consumo, abrangendo desde os registros mais atuais até dados históricos, para prever a rentabilidade futura dos clientes.\nAvaliação da Performance em M+1: O objetivo mencionado no texto é avaliar a performance no mês seguinte (M+1). Isso sugere que buscamos prever a rentabilidade dos clientes no mês subsequente.\nValidação Out-of-Time (OOT): Para a validação da performance do modelo, propomos realizar um deslocamento (shift) de 1 mês à frente para o conjunto de teste. Essa abordagem permitirá avaliar a eficácia do modelo na previsão da rentabilidade futura de maneira robusta.","metadata":{"_uuid":"99141e08-f63a-40c9-beb6-1e5c643e8ea4","_cell_guid":"313f5d96-89a5-4d6b-8eb4-2a51ac7ac530","trusted":true,"collapsed":false,"id":"UoM7aoeM6pW0","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# B. Análise Exploratória\nAvaliação de Volumetria\n\nModelo Atualizado: Para garantir que nosso modelo esteja alinhado com o cenário atual da companhia, utilizaremos a target referente ao mês de 201612 e avaliaremos a performance no mês de 201701.\n1.1. É importante observar que os dados de cadastro não estão disponíveis a partir de 201701. Portanto, descartaremos os dados de consumo e pagamento referentes a 201702.\n\n1.2. Os dados de pagamento nem sempre são registrados mensalmente, uma vez que alguns clientes optam pelo pagamento anual. Para resolver esse problema, filtraremos a última entrada de cada cliente na tabela de pagamentos, assegurando que tenhamos a informação mais recente disponível.\n\n1.3. Identificamos também a presença de clientes sem assinatura que estão ouvindo música. Precisamos abordar esses casos de forma adequada. Uma questão a considerar é se o preço para esses clientes deve ser considerado como 0.","metadata":{"_uuid":"5fe5ec5e-6d81-4aa0-89fb-5341e3a56cf7","_cell_guid":"d6b654fa-3bdf-41c8-985c-352326b7011a","trusted":true,"collapsed":false,"id":"BURUGLsS6pW0","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Corrigindo o cálculo de min e max para `msno` na base members_pd\nprint(members_pd.groupby(\"safra\")[\"msno\"].min(), members_pd.groupby(\"safra\")[\"msno\"].max())","metadata":{"_uuid":"9874df41-e91d-43de-a32d-412cb3bdaa49","_cell_guid":"f08f9a22-9df6-48b2-ab13-10b1e75a440a","trusted":true,"id":"r4PCPIrv6pW1","outputId":"cf92e12e-bea0-4c53-e04f-808eda92143f","execution":{"iopub.status.busy":"2024-11-03T14:01:25.336130Z","iopub.execute_input":"2024-11-03T14:01:25.337288Z","iopub.status.idle":"2024-11-03T14:03:00.904911Z","shell.execute_reply.started":"2024-11-03T14:01:25.337237Z","shell.execute_reply":"2024-11-03T14:03:00.903647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Corrigindo o cálculo de min e max para `msno` na base logs_pd\nprint(logs_pd.groupby(\"safra\")[\"msno\"].min(), logs_pd.groupby(\"safra\")[\"msno\"].max())","metadata":{"_uuid":"171da4db-d835-457b-a240-db02db1dd56b","_cell_guid":"69009192-21ad-4b09-b86c-bd9ded35f425","trusted":true,"collapsed":false,"id":"rr_xdLs46pW1","outputId":"15fc244c-39df-442c-a87c-f0ff9329c339","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:03:04.443132Z","iopub.execute_input":"2024-11-03T14:03:04.443641Z","iopub.status.idle":"2024-11-03T14:03:33.748491Z","shell.execute_reply.started":"2024-11-03T14:03:04.443592Z","shell.execute_reply":"2024-11-03T14:03:33.747251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Corrigindo o cálculo de min e max para `msno` na base transactions_pd\nprint(transactions_pd.groupby(\"safra\")[\"msno\"].min(), transactions_pd.groupby(\"safra\")[\"msno\"].max())","metadata":{"_uuid":"dc3889dd-7928-4841-a464-afe91e54c7c1","_cell_guid":"86db280d-6e8f-40f4-95e2-59865206df48","trusted":true,"collapsed":false,"id":"Q0U2YRHW6pW2","outputId":"2d8d02c3-1513-485a-de3f-00f51344a321","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:03:33.751148Z","iopub.execute_input":"2024-11-03T14:03:33.751686Z","iopub.status.idle":"2024-11-03T14:03:52.550390Z","shell.execute_reply.started":"2024-11-03T14:03:33.751628Z","shell.execute_reply":"2024-11-03T14:03:52.549132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Construção da Master de Treino\nCompreendendo melhor as informações disponíveis, a volumetria e a distribuição dos dados, utilizaremos um histórico de 3 meses do cliente para prever a possibilidade de churn nos 3 meses seguintes.\n\nPara isso, é essencial construir a master de treino e realizar ajustes necessários de feature engineering. Essa etapa incluirá a agregação de dados relevantes, a criação de variáveis preditivas e a aplicação de transformações adequadas nas variáveis existentes, a fim de melhorar a capacidade do modelo de identificar padrões que indiquem a evasão de clientes.","metadata":{"_uuid":"26f8983f-98dd-4613-88bb-18991c649e99","_cell_guid":"dc06c216-b3a5-4586-ab5c-017ffb7b04d2","trusted":true,"collapsed":false,"id":"9df7ozd16pW3","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\n\n# Lista das safras que serão utilizadas\nsafras = ['201612', '201609', '201608', '201607']  # Últimos 3 meses\n\n# Inicializando uma lista para armazenar as tabelas unidas\nall_join_tables = []\n\nfor safra in safras:\n    # Filtrando os dados para a safra atual\n    transactions_pd_target_full = transactions_pd[transactions_pd['safra'] <= int(safra)].copy()\n    transactions_pd_train = transactions_pd[transactions_pd['safra'] < int(safra)].copy()\n\n    # Selecionando a última entrada de cada cliente na tabela de transações\n    transactions_pd_train['rownum'] = transactions_pd_train.sort_values(by=['safra', 'msno'], ascending=False)\\\n        .groupby('msno', sort=False).cumcount().add(1)\n    transactions_pd_date = transactions_pd_train[transactions_pd_train['rownum'] == 1].drop('rownum', axis=1)\n\n    transactions_pd_target_full['rownum'] = transactions_pd_target_full.sort_values(by=['safra', 'msno'], ascending=False)\\\n        .groupby('msno', sort=False).cumcount().add(1)\n    transactions_pd_target = transactions_pd_target_full[transactions_pd_target_full['rownum'] == 1].drop('rownum', axis=1)\n\n    # Filtrando logs para a safra atual e os meses anteriores\n    members_pd_date = members_pd[members_pd['safra'] == safra]\n    logs_pd_date_target = logs_pd[logs_pd['safra'] == int(safra)]\n    logs_pd_date_3 = logs_pd[logs_pd['safra'] == int(safra) - 1]\n    logs_pd_date_2 = logs_pd[logs_pd['safra'] == int(safra) - 2]\n    logs_pd_date_1 = logs_pd[logs_pd['safra'] == int(safra) - 3]\n\n    # Filtrando apenas clientes que ouviram música ou pagaram fatura\n    join_tables_target = (members_pd_date.set_index('msno')\n                          .join(logs_pd_date_target.set_index('msno'), how='inner', rsuffix='_logs_target')\n                          .join(logs_pd_date_3.set_index('msno'), how='inner', rsuffix='_logs_m1')\n                          .join(logs_pd_date_2.set_index('msno'), how='inner', rsuffix='_logs_m2')\n                          .join(logs_pd_date_1.set_index('msno'), how='inner', rsuffix='_logs_m3')\n                          .join(transactions_pd_target.set_index('msno'), how='inner', rsuffix='_transactions_target')\n                          .join(transactions_pd_date.set_index('msno'), how='inner', rsuffix='_transactions_m1'))\n\n    # Adicionando a tabela atual à lista de resultados\n    all_join_tables.append(join_tables_target)\n\n# Concatenando todas as tabelas unidas em um único DataFrame\njoin_tables = pd.concat(all_join_tables)","metadata":{"_uuid":"ef71d156-3aca-4707-926a-99d47ceead9f","_cell_guid":"cdb95d73-0369-41c1-aca9-4158c4df1d57","trusted":true,"collapsed":false,"id":"M6h1plmP6pW3","outputId":"a1f719ad-9f28-4a6a-d6fb-bf3417554ed0","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:03:52.551970Z","iopub.execute_input":"2024-11-03T14:03:52.552369Z","iopub.status.idle":"2024-11-03T14:15:04.202828Z","shell.execute_reply.started":"2024-11-03T14:03:52.552326Z","shell.execute_reply":"2024-11-03T14:15:04.201105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualizando as primeiras linhas da tabela unida\njoin_tables.head()","metadata":{"_uuid":"5ca72de4-f94f-4b4d-a506-93b716d51a56","_cell_guid":"3776ab46-8f2b-4f7e-8864-f112cb7e1e5b","trusted":true,"collapsed":false,"id":"bQ1QGokW6pW6","outputId":"ec76cdb8-a350-4d6c-8e83-7a7743d93239","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:20:57.113895Z","iopub.execute_input":"2024-11-03T14:20:57.114740Z","iopub.status.idle":"2024-11-03T14:20:57.190913Z","shell.execute_reply.started":"2024-11-03T14:20:57.114660Z","shell.execute_reply":"2024-11-03T14:20:57.189725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Novas variáveis\n\n* tempo desde registro\n* Audição (soma)\n* Razão audição","metadata":{"_uuid":"609d7f57-f5eb-4049-909f-08afeeb67235","_cell_guid":"cf8c37fc-6da4-42e7-aa0f-67d91979fdc8","trusted":true,"collapsed":false,"id":"_77pCaaB6pW6","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Convertendo as colunas de data para datetime\njoin_tables.registration_init_time = pd.to_datetime(join_tables.registration_init_time, format='%Y%m%d', errors='coerce')\njoin_tables.safra = pd.to_datetime(join_tables.safra, format='%Y%m', errors='coerce')\n\n# Calculando o tempo desde o registro\njoin_tables['registration_ate_hoje'] = (join_tables.registration_init_time.fillna(0).astype(int) - \n                                          join_tables.safra.fillna(0).astype(int))\njoin_tables['registration_ate_hoje'] = np.where(join_tables['registration_ate_hoje'] < 0, 0, join_tables['registration_ate_hoje'])\n","metadata":{"_uuid":"80ab8c50-8ad1-4f90-bbcf-85d4df102395","_cell_guid":"fb162a21-1b48-467e-a250-d1bdbff4464a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:21:02.352874Z","iopub.execute_input":"2024-11-03T14:21:02.353918Z","iopub.status.idle":"2024-11-03T14:21:06.081067Z","shell.execute_reply.started":"2024-11-03T14:21:02.353867Z","shell.execute_reply":"2024-11-03T14:21:06.079783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Corrigindo colunas de dias até expirar as transações\njoiddn_tables['transaction_date_ate_expire_transactions_m1'] = np.where(join_tables['transaction_date_ate_expire_transactions_m1'] < 0, 0, join_tables['transaction_date_ate_expire_transactions_m1'])\njoin_tables['transaction_date_ate_expire'] = np.where(join_tables['transaction_date_ate_expire'] < 0, 0, join_tables['transaction_date_ate_expire'])\n","metadata":{"_uuid":"b27797c8-b32e-4b1f-bb70-77079918d197","_cell_guid":"7e06784f-1a92-49ce-bfec-35eec153912a","trusted":true,"id":"VcwI0ppR6pW7","execution":{"iopub.status.busy":"2024-11-03T14:23:34.213317Z","iopub.execute_input":"2024-11-03T14:23:34.214394Z","iopub.status.idle":"2024-11-03T14:23:34.340888Z","shell.execute_reply.started":"2024-11-03T14:23:34.214343Z","shell.execute_reply":"2024-11-03T14:23:34.339250Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Soma dos segundos ouvidos nos últimos meses\njoin_tables['total_secs_ult_meses'] = (join_tables.total_secs + \n                                        join_tables.total_secs_logs_m1 + \n                                        join_tables.total_secs_logs_m2 + \n                                        join_tables.total_secs_logs_m3)\n\n# Razão de audições\njoin_tables['razao_secs_ult_mes'] = join_tables.total_secs / join_tables.total_secs_logs_m1\njoin_tables['razao_secs_ult_3_mes'] = join_tables.total_secs / (join_tables.total_secs_logs_m1 + \n                                                               join_tables.total_secs + \n                                                               join_tables.total_secs_logs_m2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:24:13.353727Z","iopub.execute_input":"2024-11-03T14:24:13.354176Z","iopub.status.idle":"2024-11-03T14:24:13.432068Z","shell.execute_reply.started":"2024-11-03T14:24:13.354136Z","shell.execute_reply":"2024-11-03T14:24:13.430838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualizar os resultados\nprint(join_tables.head())","metadata":{"_uuid":"76f99fc4-1c74-46db-893e-47cb79028a32","_cell_guid":"f04280a5-9d72-4439-aa9f-b303ed2510db","trusted":true,"collapsed":false,"id":"dGxv6m8o6pYf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:24:16.907354Z","iopub.execute_input":"2024-11-03T14:24:16.907842Z","iopub.status.idle":"2024-11-03T14:24:16.947278Z","shell.execute_reply.started":"2024-11-03T14:24:16.907800Z","shell.execute_reply":"2024-11-03T14:24:16.945777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# D. Construção da Target Final\nNa etapa final, definiremos a lógica para a construção da variável target que irá indicar o churn dos clientes. Consideraremos um cliente como churn se ele atender a pelo menos uma das seguintes condições:\n\nCliente Inativo: O cliente não está ativo em sua assinatura.\nCliente Cancelado: O cliente optou por cancelar sua assinatura.\nPremissa de Expiração: A data da última atividade do cliente deve ser menor ou igual à data de expiração de sua assinatura.\nEssas definições nos permitirão criar uma target clara e objetiva, que servirá como base para o treinamento do modelo de previsão de churn.","metadata":{"_uuid":"9a57d00d-775c-43da-93a2-189ee3d16617","_cell_guid":"9dea1d95-4af1-4824-bb22-6dc34a18aeac","trusted":true,"collapsed":false,"id":"-TkfijWT6pW9","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\n\n# Definindo a variável target 'is_churn' conforme as condições de churn\njoin_tables['is_churn'] = np.where(\n    (join_tables['is_ativo'] == '0') |  # Cliente não está ativo\n    (join_tables['is_ativo'] != 1) |    # Cliente não está ativo (versão numérica)\n    (join_tables['is_cancel'] == '1') | # Cliente cancelou o plano\n    (join_tables['is_cancel_transactions_m1'] == '1'), # Cancelamento no mês anterior\n    1, # Indica churn\n    0  # Indica não churn\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:24:36.564208Z","iopub.execute_input":"2024-11-03T14:24:36.564725Z","iopub.status.idle":"2024-11-03T14:24:38.596188Z","shell.execute_reply.started":"2024-11-03T14:24:36.564659Z","shell.execute_reply":"2024-11-03T14:24:38.594920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verificando a distribuição e estatísticas dos dados por 'is_churn'\nprint(join_tables.groupby('is_churn').describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:24:43.176384Z","iopub.execute_input":"2024-11-03T14:24:43.176869Z","iopub.status.idle":"2024-11-03T14:24:55.737163Z","shell.execute_reply.started":"2024-11-03T14:24:43.176825Z","shell.execute_reply":"2024-11-03T14:24:55.735714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checando valores nulos para entender a qualidade dos dados\nprint(join_tables.isna().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:25:04.394168Z","iopub.execute_input":"2024-11-03T14:25:04.394719Z","iopub.status.idle":"2024-11-03T14:25:09.432002Z","shell.execute_reply.started":"2024-11-03T14:25:04.394657Z","shell.execute_reply":"2024-11-03T14:25:09.430491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Excluindo variaveis que formaram a target e podem gerar leakeage","metadata":{"_uuid":"78bd2a24-8e7e-4e04-9a02-7e72a05444a2","_cell_guid":"ad33822b-1a68-4a3a-b14e-9c55027206e9","trusted":true,"collapsed":false,"id":"KZN5XjF06pXK","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Removendo as variáveis que podem gerar leakage\njoin_tables.drop(['is_cancel_transactions_m1', 'is_cancel', 'is_ativo'], axis=1, inplace=True)\n","metadata":{"_uuid":"22994af2-fad3-46c7-a201-73096425391a","_cell_guid":"b9d64af4-45fd-48fc-b857-9f934b38083e","trusted":true,"collapsed":false,"id":"-BV5w3Cp6pXL","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:25:12.432972Z","iopub.execute_input":"2024-11-03T14:25:12.433432Z","iopub.status.idle":"2024-11-03T14:25:13.518576Z","shell.execute_reply.started":"2024-11-03T14:25:12.433390Z","shell.execute_reply":"2024-11-03T14:25:13.517430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verificando se as colunas foram removidas corretamente\nprint(\"Colunas atuais:\", join_tables.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:25:30.391065Z","iopub.execute_input":"2024-11-03T14:25:30.391615Z","iopub.status.idle":"2024-11-03T14:25:30.399546Z","shell.execute_reply.started":"2024-11-03T14:25:30.391558Z","shell.execute_reply":"2024-11-03T14:25:30.398310Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Calculando a taxa de churn\nchurn_rate = sum(join_tables['is_churn']) / len(join_tables['is_churn'])\nprint(\"Taxa de churn:\", churn_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:25:37.083793Z","iopub.execute_input":"2024-11-03T14:25:37.084233Z","iopub.status.idle":"2024-11-03T14:25:37.521995Z","shell.execute_reply.started":"2024-11-03T14:25:37.084194Z","shell.execute_reply":"2024-11-03T14:25:37.520626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"dd","metadata":{"_uuid":"ee2f2f18-9a06-4d9a-84e6-246c9e753a9c","_cell_guid":"10f7a029-0fe6-439d-9ed3-073aff1bddd0","trusted":true,"collapsed":false,"id":"rDY-o6tE6pXM","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Temos 9% dos clientes em churn, será necessário estratificar","metadata":{"_uuid":"a574579d-2ea5-4d8c-86f1-0081bac6cc24","_cell_guid":"f8462028-a760-46c8-ae3b-7df2a746131a","trusted":true,"collapsed":false,"id":"c-xe0y6q6pXO","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### Dropando variável de genero e data","metadata":{"_uuid":"6f9a6b30-282e-4010-845c-4723ca642475","_cell_guid":"77ae4a4f-a2f8-4d57-9632-4cb4217e89ed","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"join_tables.drop('gender',axis =1, inplace= True)","metadata":{"_uuid":"53c0ea19-24dd-4d7a-b041-af2edb0565cb","_cell_guid":"ba5b9674-627a-48a2-9a28-4a357f66540b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:26:51.076511Z","iopub.execute_input":"2024-11-03T14:26:51.077548Z","iopub.status.idle":"2024-11-03T14:26:52.544865Z","shell.execute_reply.started":"2024-11-03T14:26:51.077492Z","shell.execute_reply":"2024-11-03T14:26:52.543648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"join_tables.drop(['registration_init_time', 'safra_logs_target', 'registered_via', 'safra_logs_m1', 'safra_logs_m2','transaction_date_transactions_m1', 'safra_transactions_m1',\n                 'safra_logs_m3', 'transaction_date', 'membership_expire_date', 'safra_transactions_target', 'membership_expire_date_transactions_m1'],axis =1, inplace= True )","metadata":{"_uuid":"26ec937f-950e-43d3-88b0-6697fce0bb42","_cell_guid":"09338dd9-c175-4499-84ae-b14bdf9fcadf","trusted":true,"collapsed":false,"id":"OVglGtyn6pXV","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:27:01.795562Z","iopub.execute_input":"2024-11-03T14:27:01.796024Z","iopub.status.idle":"2024-11-03T14:27:02.722045Z","shell.execute_reply.started":"2024-11-03T14:27:01.795979Z","shell.execute_reply":"2024-11-03T14:27:02.720973Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# E. Amostragem\n### Explicação:\n- A função `calcular_tamanho_amostra()` é usada para determinar o tamanho mínimo da amostra necessário com base no nível de confiança, proporção estimada e margem de erro.\n- A função `gerar_amostra_dataset()` gera uma amostra do dataset usando o tamanho de amostra calculado. Para garantir que não tentamos amostrar mais do que o número total de observações no dataset, usamos `min(tamanho_amostra, len(dataset))`.\n- O `random_state=42` garante que a amostragem seja reproduzível.\nEsse código retorna a amostra do dataset com o número de confiança especificado.","metadata":{"_uuid":"70ba5c13-6ce4-4aa7-aa3d-5fe4fa80f827","_cell_guid":"24ed82a9-f062-46fe-8cb0-ea09a68f136a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Garantindo que as colunas de valor pago estejam no formato inteiro\njoin_tables['actual_amount_paid'] = join_tables['actual_amount_paid'].astype(int)\njoin_tables['actual_amount_paid_transactions_m1'] = join_tables['actual_amount_paid_transactions_m1'].astype(int)\n","metadata":{"_uuid":"bda29b8e-2cc1-48cc-81a5-46f05676999c","_cell_guid":"a83cc03d-3c8b-46ed-8d75-af255459b729","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:28:34.762279Z","iopub.execute_input":"2024-11-03T14:28:34.762820Z","iopub.status.idle":"2024-11-03T14:28:35.670186Z","shell.execute_reply.started":"2024-11-03T14:28:34.762773Z","shell.execute_reply":"2024-11-03T14:28:35.669086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nimport pandas as pd\n\n# Função para calcular o tamanho mínimo da amostra\ndef calcular_tamanho_amostra(Z, p, E):\n    n = (Z**2 * p * (1 - p)) / (E**2)\n    return math.ceil(n)\n\n# Função para gerar a amostra do dataset\ndef gerar_amostra_dataset(dataset, Z, p, E):\n    # Calcular o tamanho da amostra\n    tamanho_amostra = calcular_tamanho_amostra(Z, p, E)\n    # Garantir que o tamanho da amostra não seja maior que o dataset original\n    tamanho_amostra = min(tamanho_amostra, len(dataset))\n    # Gerar a amostra\n    amostra = dataset.sample(n=tamanho_amostra, random_state=42)\n    return amostra\n\n# Parâmetros de confiança\nZ = 1.96  # 95% de nível de confiança\np = 0.5   # Proporção populacional estimada\nE = 0.05  # Margem de erro de 5%\n\n# Gerar a amostra\namostra = gerar_amostra_dataset(join_tables, Z, p, E)","metadata":{"_uuid":"2ce8b034-8896-47fd-8cd1-477bcec6f0d2","_cell_guid":"89d8e186-711d-4b7e-a980-d8762f6d7f4a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:28:50.579524Z","iopub.execute_input":"2024-11-03T14:28:50.579998Z","iopub.status.idle":"2024-11-03T14:28:50.744000Z","shell.execute_reply.started":"2024-11-03T14:28:50.579955Z","shell.execute_reply":"2024-11-03T14:28:50.742605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verificando o tamanho da amostra gerada\namostra.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:29:26.627066Z","iopub.execute_input":"2024-11-03T14:29:26.627577Z","iopub.status.idle":"2024-11-03T14:29:26.689969Z","shell.execute_reply.started":"2024-11-03T14:29:26.627529Z","shell.execute_reply":"2024-11-03T14:29:26.688571Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verificando o tamanho da amostra gerada\nprint(\"Tamanho da amostra:\", len(amostra))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:29:39.467988Z","iopub.execute_input":"2024-11-03T14:29:39.468966Z","iopub.status.idle":"2024-11-03T14:29:39.474768Z","shell.execute_reply.started":"2024-11-03T14:29:39.468913Z","shell.execute_reply":"2024-11-03T14:29:39.473554Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# G. Testes de multicolinearidade\n- Spearman: Numéricas\n- MCC: Categoricas","metadata":{"_uuid":"882adcfd-68d6-483d-bcd2-b57721a0522a","_cell_guid":"fb0bb74b-055b-4123-9f0d-4ee621784ca8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import spearmanr\nfrom sklearn.metrics import matthews_corrcoef\nfrom itertools import combinations\n\n# Função para calcular a correlação de Spearman para variáveis numéricas\ndef spearman_corr(df, threshold=0.9):\n    corr_matrix = df.corr(method='spearman')\n    drop_cols = set()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold:\n                drop_cols.add(corr_matrix.columns[i])\n    return drop_cols\n\n# Função para calcular o coeficiente MCC entre variáveis categóricas\ndef mcc_corr(df, threshold=0.9):\n    drop_cols = set()\n    for col1, col2 in combinations(df.columns, 2):\n        try:\n            mcc_value = matthews_corrcoef(df[col1], df[col2])\n            if abs(mcc_value) > threshold:\n                drop_cols.add(col1)  # Remover col1 se altamente correlacionado\n        except ValueError:\n            continue  # Ignora colunas com valores únicos\n    return drop_cols\n\n# Função principal para identificar e remover colinearidades\ndef remove_colinearity(df, num_threshold=0.9, cat_threshold=0.9):\n    # Separar variáveis numéricas e categóricas\n    num_df = df.select_dtypes(include=[np.number])\n    cat_df = df.select_dtypes(include=[object, 'category'])\n    # Verificar colinearidade em variáveis numéricas\n    num_cols_to_drop = spearman_corr(num_df, num_threshold)\n    # Verificar colinearidade em variáveis categóricas\n    cat_cols_to_drop = mcc_corr(cat_df, cat_threshold)\n    # Remover colunas altamente correlacionadas\n    df_cleaned = df.drop(columns=list(num_cols_to_drop) + list(cat_cols_to_drop))\n    return df_cleaned\n","metadata":{"_uuid":"b7fe655f-e0b7-480b-92d6-d9592085809f","_cell_guid":"3e9ef25b-cbad-40d4-8c9c-d7ac37e51dd9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:32:45.243000Z","iopub.execute_input":"2024-11-03T14:32:45.243493Z","iopub.status.idle":"2024-11-03T14:32:45.257086Z","shell.execute_reply.started":"2024-11-03T14:32:45.243453Z","shell.execute_reply":"2024-11-03T14:32:45.255793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Aplicando ao dataset amostra\ndataset_cleaned = remove_colinearity(amostra, num_threshold=0.9, cat_threshold=0.9)\n","metadata":{"_uuid":"a4f03a86-b569-4f65-a44d-3be016ee3065","_cell_guid":"dd0927c0-63b0-4a72-95fc-af7494a4c609","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:33:13.521435Z","iopub.execute_input":"2024-11-03T14:33:13.522604Z","iopub.status.idle":"2024-11-03T14:33:13.666634Z","shell.execute_reply.started":"2024-11-03T14:33:13.522534Z","shell.execute_reply":"2024-11-03T14:33:13.665296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Atualizando join_tables para refletir as colunas selecionadas\njoin_tables = join_tables[dataset_cleaned.columns]","metadata":{"_uuid":"d88ee166-1920-44ab-9baf-c79eb61c8256","_cell_guid":"6c1cb1e7-9efa-4f22-bb14-7d14e6dcf2ac","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:33:17.125440Z","iopub.execute_input":"2024-11-03T14:33:17.126071Z","iopub.status.idle":"2024-11-03T14:33:17.992700Z","shell.execute_reply.started":"2024-11-03T14:33:17.126008Z","shell.execute_reply":"2024-11-03T14:33:17.991412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualizando o resultado\njoin_tables.head()","metadata":{"_uuid":"97ca0707-51ed-4c1c-af96-826dd438133c","_cell_guid":"95efbe97-1fbe-43e1-98a3-56907d7bb105","trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"execution":{"iopub.status.busy":"2024-11-03T14:33:27.997360Z","iopub.execute_input":"2024-11-03T14:33:27.997879Z","iopub.status.idle":"2024-11-03T14:33:28.043826Z","shell.execute_reply.started":"2024-11-03T14:33:27.997831Z","shell.execute_reply":"2024-11-03T14:33:28.042429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# E. Divisão de treino e teste\nCom as variaveis importantes, dividiremos nossa base entre treino e teste. Como estratégia para qualdiade, dividiremos nossa base entre treino, teste e validação. Além disso,usaremos a proporção 60%/40%.\n\nAnterior à divisão excluiremos a variável de *genero* e faremos check de colinariedade","metadata":{"_uuid":"d6d9282c-ba4b-4cc0-97ac-1c76630f3189","_cell_guid":"0a98673f-ff0e-48de-9eb4-6f56cbfaa1eb","trusted":true,"collapsed":false,"id":"DTbhjcI_6pXg","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Preparando os dados\njoin_tables = join_tables.reset_index(drop=True)\n\n# Excluindo a variável 'gender' e outras colunas irrelevantes, verificando também tipos de dados\njoin_tables = join_tables.drop(columns=['gender', 'msno', 'safra'], errors='ignore')\njoin_tables['payment_plan_days_transactions_m1'] = join_tables['payment_plan_days_transactions_m1'].astype(int)\njoin_tables['plan_list_price_transactions_m1'] = join_tables['plan_list_price_transactions_m1'].astype(int)\njoin_tables['is_auto_renew_transactions_m1'] = join_tables['is_auto_renew_transactions_m1'].astype(int)\njoin_tables['payment_method_id_transactions_m1'] = join_tables['payment_method_id_transactions_m1'].astype(int)\njoin_tables['city'] = join_tables['city'].astype(int)\njoin_tables['bd'] = join_tables['bd'].astype(int)\n\n# Verificação de valores nulos\nmissing_values = join_tables.isna().sum().sum()\nprint(f'Total de valores nulos: {missing_values}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:38:02.819303Z","iopub.execute_input":"2024-11-03T14:38:02.820112Z","iopub.status.idle":"2024-11-03T14:38:10.655222Z","shell.execute_reply.started":"2024-11-03T14:38:02.820049Z","shell.execute_reply":"2024-11-03T14:38:10.653446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"O erro indica que há valores nulos","metadata":{}},{"cell_type":"code","source":"# Preenchendo valores nulos com 0 nas colunas que serão convertidas para int\njoin_tables['payment_plan_days_transactions_m1'].fillna(0, inplace=True)\njoin_tables['plan_list_price_transactions_m1'].fillna(0, inplace=True)\njoin_tables['is_auto_renew_transactions_m1'].fillna(0, inplace=True)\njoin_tables['payment_method_id_transactions_m1'].fillna(0, inplace=True)\njoin_tables['city'].fillna(0, inplace=True)\njoin_tables['bd'].fillna(0, inplace=True)\n\n# Convertendo colunas para inteiro\njoin_tables['payment_plan_days_transactions_m1'] = join_tables['payment_plan_days_transactions_m1'].astype(int)\njoin_tables['plan_list_price_transactions_m1'] = join_tables['plan_list_price_transactions_m1'].astype(int)\njoin_tables['is_auto_renew_transactions_m1'] = join_tables['is_auto_renew_transactions_m1'].astype(int)\njoin_tables['payment_method_id_transactions_m1'] = join_tables['payment_method_id_transactions_m1'].astype(int)\njoin_tables['city'] = join_tables['city'].astype(int)\njoin_tables['bd'] = join_tables['bd'].astype(int)\n\n# Verificação de valores nulos\nmissing_values = join_tables.isna().sum().sum()\nprint(f'Total de valores nulos após preenchimento: {missing_values}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:39:46.687384Z","iopub.execute_input":"2024-11-03T14:39:46.687932Z","iopub.status.idle":"2024-11-03T14:39:46.974275Z","shell.execute_reply.started":"2024-11-03T14:39:46.687886Z","shell.execute_reply":"2024-11-03T14:39:46.972944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Preparando os dados\njoin_tables = join_tables.reset_index(drop=True)\n\n# Excluindo a variável 'gender' e outras colunas irrelevantes, verificando também tipos de dados\njoin_tables = join_tables.drop(columns=['gender', 'msno', 'safra'], errors='ignore')\njoin_tables['payment_plan_days_transactions_m1'] = join_tables['payment_plan_days_transactions_m1'].astype(int)\njoin_tables['plan_list_price_transactions_m1'] = join_tables['plan_list_price_transactions_m1'].astype(int)\njoin_tables['is_auto_renew_transactions_m1'] = join_tables['is_auto_renew_transactions_m1'].astype(int)\njoin_tables['payment_method_id_transactions_m1'] = join_tables['payment_method_id_transactions_m1'].astype(int)\njoin_tables['city'] = join_tables['city'].astype(int)\njoin_tables['bd'] = join_tables['bd'].astype(int)\n\n# Verificação de valores nulos\nmissing_values = join_tables.isna().sum().sum()\nprint(f'Total de valores nulos: {missing_values}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:40:09.367387Z","iopub.execute_input":"2024-11-03T14:40:09.367908Z","iopub.status.idle":"2024-11-03T14:40:10.515951Z","shell.execute_reply.started":"2024-11-03T14:40:09.367864Z","shell.execute_reply":"2024-11-03T14:40:10.514687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separação de features (X) e target (y)\nX = join_tables.drop(columns=['is_churn'])\ny = join_tables['is_churn']","metadata":{"_uuid":"bebc1d24-176f-46f4-87fc-78410883e41d","_cell_guid":"0d1f2063-fce7-41c5-a187-3bb596c62103","trusted":true,"collapsed":false,"id":"14fUrlHW6pXR","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:40:20.197839Z","iopub.execute_input":"2024-11-03T14:40:20.199042Z","iopub.status.idle":"2024-11-03T14:40:20.506757Z","shell.execute_reply.started":"2024-11-03T14:40:20.198990Z","shell.execute_reply":"2024-11-03T14:40:20.505520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dividindo os dados em treino (60%) e temp (40%)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n\n# Dividindo temp (40%) em validação (40% de temp) e teste (60% de temp)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.6, random_state=42, stratify=y_temp)","metadata":{"_uuid":"98022139-eaed-452f-9b28-3f262f4c0ea9","_cell_guid":"a49bfc6b-394d-48c2-b610-d527dfbe6e9d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:40:33.146404Z","iopub.execute_input":"2024-11-03T14:40:33.146937Z","iopub.status.idle":"2024-11-03T14:40:38.494452Z","shell.execute_reply.started":"2024-11-03T14:40:33.146882Z","shell.execute_reply":"2024-11-03T14:40:38.493266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Exibindo as dimensões dos conjuntos de dados\nprint(f\"Dimensões do conjunto de treino: {X_train.shape}\")","metadata":{"_uuid":"0cef5d0a-85de-4dfc-829c-52c9b940d9bb","_cell_guid":"91d788fd-c212-4eaa-9e23-e269054e1117","trusted":true,"collapsed":false,"id":"DEjZAzCc6pXS","outputId":"8e8e086e-ad3b-4080-d607-f88aeaed3098","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:40:45.430290Z","iopub.execute_input":"2024-11-03T14:40:45.430800Z","iopub.status.idle":"2024-11-03T14:40:45.437327Z","shell.execute_reply.started":"2024-11-03T14:40:45.430757Z","shell.execute_reply":"2024-11-03T14:40:45.436140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Dimensões do conjunto de validação: {X_val.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:40:52.206437Z","iopub.execute_input":"2024-11-03T14:40:52.206974Z","iopub.status.idle":"2024-11-03T14:40:52.213226Z","shell.execute_reply.started":"2024-11-03T14:40:52.206924Z","shell.execute_reply":"2024-11-03T14:40:52.212010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Dimensões do conjunto de teste: {X_test.shape}\")","metadata":{"_uuid":"3df59c0b-1494-4434-8eac-eabacd5f1f90","_cell_guid":"26bd1ac9-c9da-4727-bd55-f5e91e01d2a6","trusted":true,"collapsed":false,"id":"bzXE3RUG6pXl","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:40:55.553130Z","iopub.execute_input":"2024-11-03T14:40:55.553603Z","iopub.status.idle":"2024-11-03T14:40:55.559961Z","shell.execute_reply.started":"2024-11-03T14:40:55.553553Z","shell.execute_reply":"2024-11-03T14:40:55.558772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conferindo as primeiras linhas do conjunto de treino\nX_train.head()","metadata":{"_uuid":"1b95075f-9ef3-4a3b-bc34-ba35d8727cdc","_cell_guid":"11df3eb7-c5bc-465e-b4dd-25c785478e2a","trusted":true,"collapsed":false,"id":"uR9DkuXf6pXm","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:40:58.704129Z","iopub.execute_input":"2024-11-03T14:40:58.704584Z","iopub.status.idle":"2024-11-03T14:40:58.747349Z","shell.execute_reply.started":"2024-11-03T14:40:58.704544Z","shell.execute_reply":"2024-11-03T14:40:58.746250Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# F. Feature Engineering\n\nEtapa necessária pré treino do modelo, iremos fazer os casts necessários, ajuste de dummies e outros.","metadata":{"_uuid":"cfe3722f-f82b-489c-9e68-ebc04c9c3b4c","_cell_guid":"9ddda5f5-4414-4530-b91c-6d09b913d545","trusted":true,"collapsed":false,"id":"YeW7Wq1m6pXO","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# 1. Identificar variáveis categóricas e numéricas\ncategorical_cols = ['payment_method_id_transactions_m1', 'city']  # Exemplo de variáveis categóricas, ajuste conforme necessário\nnumerical_cols = [col for col in X_train.columns if col not in categorical_cols]\n\n# 2. Converting categorical variables to category type\nfor col in categorical_cols:\n    X_train[col] = X_train[col].astype('category')\n    X_val[col] = X_val[col].astype('category')\n    X_test[col] = X_test[col].astype('category')\n\n# 3. One-hot encoding para variáveis categóricas\nX_train = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\nX_val = pd.get_dummies(X_val, columns=categorical_cols, drop_first=True)\nX_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n\n# Certificar que as colunas de treino, validação e teste estão alinhadas após o encoding\nX_val = X_val.reindex(columns=X_train.columns, fill_value=0)\nX_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n\n# 4. Padronização das variáveis numéricas\nscaler = StandardScaler()\nX_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\nX_val[numerical_cols] = scaler.transform(X_val[numerical_cols])\nX_test[numerical_cols] = scaler.transform(X_test[numerical_cols])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:44:19.436423Z","iopub.execute_input":"2024-11-03T14:44:19.437477Z","iopub.status.idle":"2024-11-03T14:44:23.543374Z","shell.execute_reply.started":"2024-11-03T14:44:19.437425Z","shell.execute_reply":"2024-11-03T14:44:23.542087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualizando as dimensões dos conjuntos após o processamento\nprint(\"Dimensões do conjunto de treino:\", X_train.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:44:27.470435Z","iopub.execute_input":"2024-11-03T14:44:27.470956Z","iopub.status.idle":"2024-11-03T14:44:27.477186Z","shell.execute_reply.started":"2024-11-03T14:44:27.470908Z","shell.execute_reply":"2024-11-03T14:44:27.475946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Dimensões do conjunto de validação:\", X_val.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:44:29.858976Z","iopub.execute_input":"2024-11-03T14:44:29.859452Z","iopub.status.idle":"2024-11-03T14:44:29.866092Z","shell.execute_reply.started":"2024-11-03T14:44:29.859407Z","shell.execute_reply":"2024-11-03T14:44:29.864936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Dimensões do conjunto de teste:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:44:31.909798Z","iopub.execute_input":"2024-11-03T14:44:31.910720Z","iopub.status.idle":"2024-11-03T14:44:31.916821Z","shell.execute_reply.started":"2024-11-03T14:44:31.910656Z","shell.execute_reply":"2024-11-03T14:44:31.915554Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# G.Feature Selection\n\nÉ interessante separar o que realmente influencia ou não no churn, para tanto, devido a termos uma base mixada (variaveis categoricas e numericas) utilizaremos uma abordagem baseada em ensemble.\n\nLightGBM é extremamente eficiente para data frames grandes.\nComo o Random Forest, ele também fornece importâncias de variáveis, mas é otimizado para desempenho em datasets grandes e com alto número de features.\nEle pode lidar bem com variáveis categóricas, e geralmente é mais rápido que Random Forest devido à sua implementação baseada em histogramas. Além disso, atende bem problemas desbalanceados como  o nosso,","metadata":{"_uuid":"dfec3a66-397b-4bb4-a53d-603eadf5aaad","_cell_guid":"086e2183-1123-4d0b-8886-69c992e257b8","trusted":true,"collapsed":false,"id":"jVqMfVP56pXR","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Definindo o modelo LightGBM para feature selection\nlgb_model = lgb.LGBMClassifier(random_state=42, n_estimators=500)\n\n# Treinando o modelo com todas as features\nlgb_model.fit(X_train, y_train)\n\n# Extraindo as importâncias das features\nfeature_importances = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': lgb_model.feature_importances_\n})\n\n# Ordenando por importância e selecionando as principais variáveis\nfeature_importances = feature_importances.sort_values(by='importance', ascending=False)\n\n# Definindo um número de features a serem mantidas\ntop_k = 20\ntop_features = feature_importances.head(top_k)['feature'].values\n\n# Selecionando as features mais importantes no conjunto de dados\nX_train_selected = X_train[top_features]\nX_val_selected = X_val[top_features]\nX_test_selected = X_test[top_features]","metadata":{"_uuid":"b18c10ed-9d73-44cf-8548-4300d2e6b3d9","_cell_guid":"8fc94450-1289-4424-9234-e23ad82f7ea8","trusted":true,"collapsed":false,"id":"OdmU5RSE6pXc","outputId":"3bbc6a6d-d02a-491c-c983-3e0ad6959705","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:47:11.604756Z","iopub.execute_input":"2024-11-03T14:47:11.605299Z","iopub.status.idle":"2024-11-03T14:49:20.761096Z","shell.execute_reply.started":"2024-11-03T14:47:11.605252Z","shell.execute_reply":"2024-11-03T14:49:20.759869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verificação das dimensões após a seleção\nprint(\"Dimensões do conjunto de treino com features selecionadas:\", X_train_selected.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:49:56.699659Z","iopub.execute_input":"2024-11-03T14:49:56.700153Z","iopub.status.idle":"2024-11-03T14:49:56.706983Z","shell.execute_reply.started":"2024-11-03T14:49:56.700112Z","shell.execute_reply":"2024-11-03T14:49:56.705508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Dimensões do conjunto de validação com features selecionadas:\", X_val_selected.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:49:59.338813Z","iopub.execute_input":"2024-11-03T14:49:59.339280Z","iopub.status.idle":"2024-11-03T14:49:59.350000Z","shell.execute_reply.started":"2024-11-03T14:49:59.339236Z","shell.execute_reply":"2024-11-03T14:49:59.348819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Dimensões do conjunto de teste com features selecionadas:\", X_test_selected.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T14:50:00.786017Z","iopub.execute_input":"2024-11-03T14:50:00.786486Z","iopub.status.idle":"2024-11-03T14:50:00.792640Z","shell.execute_reply.started":"2024-11-03T14:50:00.786432Z","shell.execute_reply":"2024-11-03T14:50:00.791440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Com as variáveis definidas vamos iniciar o treinamento do algoritmo","metadata":{"_uuid":"7774f45b-9479-441e-963f-a66141f37e96","_cell_guid":"a87e84cb-c16e-4ce4-9836-ee0745fac012","trusted":true,"collapsed":false,"id":"p1P7F7np6pXf","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# H. Treinando o algoritmo\n\nPara o treinamento optamos por testar diferentes classes, cujos principais pontos positivos e negativos estão descritos abaixo:\n\n\n### 1. **LightGBM**\n  - **Por que usar:**  Muito rápido e eficiente em termos de uso de memória, especialmente em conjuntos de dados grandes, permite ajuste fino de muitos parâmetros, como a profundidade da árvore e o número de folhas, para melhorar o desempenho e lida bem com conjuntos de dados esparsos, o que é comum em problemas com muitas variáveis categóricas.\n  - **Por que não usar LightGBM:** tem muitoshiperparâmetros que precisam ser ajustados corretamente, o que pode ser complexo e demorado e como outros algoritmos de gradient boosting, pode ser suscetível a overfitting, especialmente em conjuntos de dados pequenos ou mal balanceados.\n\n### 2. **XGBoost (XGBClassifier)**\n  - **Por que usar:** É  uma técnica de boosting extremamente eficiente e poderosa que geralmente supera muitos outros algoritmos em termos de performance em problemas de classificação. Ele é bom em capturar interações complexas e pode lidar com dados desbalanceados, que são comuns em problemas de churn.\n  - **Por que não usar:** Similar ao Random Forest, XGBoost pode ser caro em termos computacionais. Além disso, requer um ajuste de hiperparâmetros mais cuidadoso para evitar overfitting, o que pode aumentar o tempo de desenvolvimento.\n\n### 3. **Random Forest**\n  - **Por que usar:** Robustex cobtra Overfiting, lida bem com dados faltantes\n  - **Por que não usar:** Propensas a Bias","metadata":{"_uuid":"c5214df4-3a9f-4d05-bda5-7c4dbe9e421e","_cell_guid":"31dc80d4-8355-4803-97d0-3487359296bf","trusted":true,"collapsed":false,"id":"X4lWxm636pXm","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Aqui está um código que implementa os ajustes para `scale_pos_weight`, `sample_weight`, e o ajuste do threshold de decisão no XGBoost para lidar com dados desbalanceados:\n### Explicação:\n1. **Ajuste do `scale_pos_weight`** (ponto 2): Calculamos a razão entre as amostras negativas e positivas no conjunto de treino para balancear o impacto das classes.\n2. **Uso de `sample_weight`** (ponto 4): Aplicamos pesos diferentes às amostras, dando mais importância às da classe minoritária.\n3. **Threshold tuning** (ponto 5): Ao invés de usar o threshold padrão de 0.5 para a probabilidade de previsão, ajustamos para 0.3 para captar mais positivos.\nEsse código pode ser adaptado ao seu problema específico, ajustando os parâmetros conforme necessário.","metadata":{"_uuid":"bad118d7-6333-474d-8912-6dab127e12eb","_cell_guid":"eb79b454-d79c-46a2-8eeb-d6fd4910c698","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Calculando scale_pos_weight\nnum_pos = np.sum(y_train == 1)  # Número de positivos\nnum_neg = np.sum(y_train == 0)  # Número de negativos\nscale_pos_weight = num_neg / num_pos","metadata":{"_uuid":"50571d03-de44-456e-9501-b39029b8149e","_cell_guid":"1bb05bb3-ed92-408a-a371-6c65b8fa03fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:54:08.043880Z","iopub.execute_input":"2024-11-03T14:54:08.045201Z","iopub.status.idle":"2024-11-03T14:54:08.060041Z","shell.execute_reply.started":"2024-11-03T14:54:08.045147Z","shell.execute_reply":"2024-11-03T14:54:08.058594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model  import LogisticRegression\n# Application of all Machine Learning methods\nMLA = [\n    LogisticRegression(class_weight='balanced',C=1),\n\n    # LightGBM\n    LGBMClassifier(scale_pos_weight=scale_pos_weight,  # Ajuste para classes desbalanceadas\n   use_label_encoder=True),\n\n    #Ensemble Methods\n    XGBClassifier( scale_pos_weight=scale_pos_weight,  # Ajuste para classes desbalanceadas\n   use_label_encoder=True),\n\n    #Random Forest\n    RandomForestClassifier(max_depth=2,class_weight = 'balanced')\n\n    ]","metadata":{"_uuid":"f0cf2ff8-efc8-4c8b-b656-f4ebfa7f853b","_cell_guid":"90ff2230-fcaa-4591-8fdd-8cc93048eab2","trusted":true,"collapsed":false,"id":"sfGk6ZqT6pXn","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:54:24.157919Z","iopub.execute_input":"2024-11-03T14:54:24.158389Z","iopub.status.idle":"2024-11-03T14:54:24.168423Z","shell.execute_reply.started":"2024-11-03T14:54:24.158343Z","shell.execute_reply":"2024-11-03T14:54:24.167230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import precision_score, recall_score, roc_curve, auc, f1_score\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn import linear_model, ensemble, tree, naive_bayes\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\nimport numpy as np\n\n\n# Inicializando as colunas do DataFrame para comparação\nMLA_columns = ['MLA used', 'Train Accuracy', 'Validation Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\nMLA_compare = pd.DataFrame(columns=MLA_columns)\n\n# Índice para linhas do DataFrame\nrow_index = 0\n\n# Loop para cada algoritmo na lista MLA\nfor alg in MLA:\n    print(f\"Evaluando {alg.__class__.__name__}...\")\n\n    # Validação Cruzada usando o conjunto de treinamento\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    val_accuracies = []\n    val_precisions = []\n    val_recalls = []\n    val_f1_scores = []\n    val_auc_scores = []\n\n    for train_index, val_index in kf.split(X_train):\n        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n\n        # Treinando o modelo\n        alg.fit(X_train_fold, y_train_fold)\n\n        # Predições e métricas no conjunto de validação\n        y_val_pred = alg.predict(X_val_fold)\n        y_val_pred_proba = alg.predict_proba(X_val_fold)[:, 1] if hasattr(alg, 'predict_proba') else y_val_pred\n\n        val_accuracies.append(alg.score(X_val_fold, y_val_fold))\n        val_precisions.append(precision_score(y_val_fold, y_val_pred))\n        val_recalls.append(recall_score(y_val_fold, y_val_pred))\n        val_f1_scores.append(f1_score(y_val_fold, y_val_pred))\n        fp, tp, _ = roc_curve(y_val_fold, y_val_pred_proba)\n        val_auc_scores.append(auc(fp, tp))\n\n    # Calculando métricas médias\n    val_accuracy = round(np.mean(val_accuracies), 4)\n    val_precision = round(np.mean(val_precisions), 4)\n    val_recall = round(np.mean(val_recalls), 4)\n    val_f1 = round(np.mean(val_f1_scores), 4)\n    val_auc = round(np.mean(val_auc_scores), 4)\n\n    # Treinamento final no conjunto de treino\n    alg.fit(X_train, y_train)\n\n    # Avaliação final no conjunto de teste\n    y_test_pred = alg.predict(X_test)\n    y_test_pred_proba = alg.predict_proba(X_test)[:, 1] if hasattr(alg, 'predict_proba') else y_test_pred\n\n    test_accuracy = round(alg.score(X_test, y_test), 4)\n    precision = precision_score(y_test, y_test_pred)\n    recall = recall_score(y_test, y_test_pred)\n    f1 = f1_score(y_test, y_test_pred)\n    fp, tp, _ = roc_curve(y_test, y_test_pred_proba)\n    auc_score = auc(fp, tp)\n\n    # Armazenando métricas no DataFrame\n    MLA_compare.loc[row_index] = [alg.__class__.__name__,\n                                   round(alg.score(X_train, y_train), 4),\n                                   val_accuracy,\n                                   test_accuracy,\n                                   precision,\n                                   recall,\n                                   f1,\n                                   auc_score]\n\n    row_index += 1","metadata":{"_uuid":"c00edd5f-39d4-4b4f-a4d9-6e28c06f3c57","_cell_guid":"c48ac929-201a-4b36-8ab8-fdcc5d29cca4","trusted":true,"collapsed":false,"id":"D0QQt0n16pXr","outputId":"13fb9092-77d5-4703-8eda-c4266be2525b","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T14:54:27.287481Z","iopub.execute_input":"2024-11-03T14:54:27.287984Z","iopub.status.idle":"2024-11-03T15:15:21.198213Z","shell.execute_reply.started":"2024-11-03T14:54:27.287940Z","shell.execute_reply":"2024-11-03T15:15:21.196297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MLA_compare.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)\nMLA_compare","metadata":{"_uuid":"b995f13d-4e82-437c-9f53-bf4b4e000d12","_cell_guid":"cd029c6e-6686-4523-9393-2e8f8751ec7d","trusted":true,"collapsed":false,"id":"9LVyOitO6pXs","outputId":"fc00cc09-7cc9-4528-f9ab-1fade2673e60","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T15:15:28.567476Z","iopub.execute_input":"2024-11-03T15:15:28.568001Z","iopub.status.idle":"2024-11-03T15:15:28.588719Z","shell.execute_reply.started":"2024-11-03T15:15:28.567953Z","shell.execute_reply":"2024-11-03T15:15:28.587208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# I. Hiperparametrização\n\nEscolhemos os parametros de redução de complexidade do modelo, regularização, subamostragem e aprendizado focando em melhorar assertividade do modelo preservando o trade-off entre viés e variância, abaixo detalho os parametros:\n\n1) Redução de complexidade - profundidade das árvores, quanto mais rasa menos propensa a overfitar\n\n2) Regularização L1 para esparsidade\n\n3) Taxa de Aprendizado\n\n4) Redução de fração de amostras na contrução da arvore","metadata":{"_uuid":"bec71ae4-ecd1-442a-89b4-95e7f2192eee","_cell_guid":"8d0a5c9a-dd40-42e5-b534-317f785594d2","trusted":true,"collapsed":false,"id":"VeP118o96pXt","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Para realizar a troca de `RandomizedSearchCV` por uma busca bayesiana, você pode utilizar a biblioteca `BayesSearchCV` do pacote `scikit-optimize`, que é uma alternativa eficiente para otimização de hiperparâmetros. O principal benefício da busca bayesiana é que ela explora o espaço de parâmetros de forma mais inteligente, ajustando as tentativas com base nos resultados anteriores.\nAqui está o código modificado para usar a busca bayesiana, e com a métrica de recall como critério de avaliação:\n\n### Principais alterações:\n1. **Biblioteca usada:** `BayesSearchCV` do `scikit-optimize`.\n2. **Espaço de parâmetros:** A sintaxe de intervalos é ligeiramente diferente, com o uso de `(min, max)` e distribuições como `'log-uniform'`.\n3. **Critério de avaliação:** O recall foi definido como métrica principal usando `make_scorer(recall_score)` e passado para o parâmetro `scoring`.\n4. **Parâmetro `n_estimators`:** O intervalo foi ampliado para permitir uma busca mais ampla.\nEssa abordagem permitirá uma busca mais eficiente, focando em otimizar o recall.","metadata":{"_uuid":"1ba8a72a-38c1-4cf8-bc49-38a2fab56832","_cell_guid":"7049e4c3-cf0e-44e4-be3b-be208227c73c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import make_scorer, recall_score\nfrom skopt import BayesSearchCV\nfrom xgboost import XGBClassifier\n\n# Definir o modelo inicial\nmodel = XGBClassifier(scale_pos_weight=scale_pos_weight, use_label_encoder=False)\n\n# Parâmetros para Busca Bayesiana (extremamente simplificados)\nparam_dist = {\n    'max_depth': (1, 2),  # Profundidade mínima\n    'reg_alpha': (0.01, 0.1),  # Regularização leve\n    'learning_rate': (0.01, 0.1),  # Taxa de aprendizado baixa\n    'n_estimators': (5, 10),  # Valor muito baixo para árvores\n    'subsample': (0.5, 1.0),  # Usar entre 50% a 100% dos dados\n    'colsample_bytree': (0.5, 1.0)  # Usar entre 50% a 100% das colunas\n}\n\n# Scorer baseado em recall\nrecall = make_scorer(recall_score)\n\n# Amostragem do conjunto de dados (1% dos dados)\nX_train_sample = X_train.sample(frac=0.01, random_state=42)  # Usar 1% dos dados\ny_train_sample = y_train.loc[X_train_sample.index]\n\n# BayesSearchCV com validação cruzada (configurado para ser extremamente rápido)\nbayes_search = BayesSearchCV(\n    estimator=model,\n    search_spaces=param_dist,\n    n_iter=1,  # Apenas uma iteração\n    cv=2,  # Validação cruzada com 2 folds\n    n_jobs=1,  # Usar apenas um núcleo para evitar sobrecarga\n    verbose=0,  # Silenciar logs\n    random_state=3,\n    scoring=recall\n)\n\n# Aplicando a busca (sem avaliação)\nbayes_search.fit(X_train_sample, y_train_sample)\n","metadata":{"_uuid":"444c1651-7156-4c05-b349-af4c40645d6e","_cell_guid":"ed0cd487-3758-443d-80f9-7666c2994b07","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T15:34:40.177366Z","iopub.execute_input":"2024-11-03T15:34:40.177935Z","iopub.status.idle":"2024-11-03T15:34:41.558551Z","shell.execute_reply.started":"2024-11-03T15:34:40.177886Z","shell.execute_reply":"2024-11-03T15:34:41.557374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Melhor modelo e parâmetros\nbest_model_bayes = bayes_search.best_estimator_\nprint(f\"Best parameters (BayesSearchCV): {bayes_search.best_params_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T15:34:56.325322Z","iopub.execute_input":"2024-11-03T15:34:56.325829Z","iopub.status.idle":"2024-11-03T15:34:56.332474Z","shell.execute_reply.started":"2024-11-03T15:34:56.325783Z","shell.execute_reply":"2024-11-03T15:34:56.331297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Best score (BayesSearchCV - Recall): {bayes_search.best_score_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T15:34:59.358774Z","iopub.execute_input":"2024-11-03T15:34:59.359966Z","iopub.status.idle":"2024-11-03T15:34:59.366057Z","shell.execute_reply.started":"2024-11-03T15:34:59.359911Z","shell.execute_reply":"2024-11-03T15:34:59.364909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Avaliação do modelo final\ny_pred_bayes = best_model_bayes.predict(X_test)\ny_pred_proba_bayes = best_model_bayes.predict_proba(X_test)[:, 1]\n\n# Métricas\naccuracy_bayes = recall_score(y_test, y_pred_bayes)\nauc_bayes = roc_auc_score(y_test, y_pred_proba_bayes)\nf1_bayes = f1_score(y_test, y_pred_bayes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T15:35:01.965113Z","iopub.execute_input":"2024-11-03T15:35:01.965767Z","iopub.status.idle":"2024-11-03T15:35:20.149096Z","shell.execute_reply.started":"2024-11-03T15:35:01.965677Z","shell.execute_reply":"2024-11-03T15:35:20.147889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Test recall (BayesSearchCV): {accuracy_bayes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T15:35:27.205618Z","iopub.execute_input":"2024-11-03T15:35:27.206134Z","iopub.status.idle":"2024-11-03T15:35:27.212462Z","shell.execute_reply.started":"2024-11-03T15:35:27.206088Z","shell.execute_reply":"2024-11-03T15:35:27.211096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Test AUC (BayesSearchCV): {auc_bayes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T15:35:29.921384Z","iopub.execute_input":"2024-11-03T15:35:29.921881Z","iopub.status.idle":"2024-11-03T15:35:29.928896Z","shell.execute_reply.started":"2024-11-03T15:35:29.921837Z","shell.execute_reply":"2024-11-03T15:35:29.927487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Test F1 Score (BayesSearchCV): {f1_bayes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T15:35:32.234966Z","iopub.execute_input":"2024-11-03T15:35:32.235960Z","iopub.status.idle":"2024-11-03T15:35:32.241718Z","shell.execute_reply.started":"2024-11-03T15:35:32.235904Z","shell.execute_reply":"2024-11-03T15:35:32.240567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Agora que descobrimos os melhores parametros para o algoritmo vamos resolver a previsão 3 meses a frente.","metadata":{"_uuid":"134ea813-3c9a-4f3a-939c-647ab63cab69","_cell_guid":"a5bfbe54-256d-41ea-b19c-5ea096b463e9","trusted":true,"collapsed":false,"id":"HtgGX4wx6pXx","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# J. Prevendo 3 meses a frente\nPor fim iremos finalizar a previsão dos clientes que nao deram churn neste mês mas que poderiam dar no proximo ou em M2, M3 com os valores otimos da busca bayesiana. \n\nCriaremos também uma função de visualização para os resultados do modelo.","metadata":{"_uuid":"979e4023-e401-4607-a5fe-34d951c63fb7","_cell_guid":"87a727a8-1e0c-4d48-a5fc-253086e58fea","trusted":true,"collapsed":false,"id":"P9kS5Tdi6pXx","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"join_tables['safra'] = pd.to_datetime(join_tables.safra, format='%Y%m', errors='coerce').dt.strftime('%Y%m')","metadata":{"_uuid":"bf9eb7fd-11ac-4005-8c86-bebc56e0133f","_cell_guid":"3074183c-714e-441a-beaf-7da722106496","trusted":true,"collapsed":false,"id":"M61YX-Zz6pXy","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T15:39:24.444214Z","iopub.execute_input":"2024-11-03T15:39:24.444783Z","iopub.status.idle":"2024-11-03T15:39:24.481436Z","shell.execute_reply.started":"2024-11-03T15:39:24.444726Z","shell.execute_reply":"2024-11-03T15:39:24.479757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Notebook 4\njoin_tables['churn_1'] = join_tables.groupby('msno')['is_churn'].shift(-1)\njoin_tables['churn_2'] = join_tables.groupby('msno')['is_churn'].shift(-2)\njoin_tables['churn_3'] = join_tables.groupby('msno')['is_churn'].shift(-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T15:43:19.478667Z","iopub.execute_input":"2024-11-03T15:43:19.479198Z","iopub.status.idle":"2024-11-03T15:43:19.608487Z","shell.execute_reply.started":"2024-11-03T15:43:19.479152Z","shell.execute_reply":"2024-11-03T15:43:19.606778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Previsão para M1\nx1m = join_tables.dropna(subset=['churn_1']).filter(feature_importance_df)\ny_1m = join_tables.dropna(subset=['churn_1'])['churn_1']\n\n# Previsão para M2\nx2m = join_tables.dropna(subset=['churn_2']).filter(feature_importance_df)\ny_2m = join_tables.dropna(subset=['churn_2'])['churn_2']\n\n# Previsão para M3\nx3m = join_tables.dropna(subset=['churn_3']).filter(feature_importance_df)\ny_3m = join_tables.dropna(subset=['churn_3'])['churn_3']\n","metadata":{"_uuid":"4f51d0c3-cd22-4dba-a827-19519a60dcfc","_cell_guid":"f9706457-ebb5-48fa-ba20-c04553f557e9","trusted":true,"collapsed":false,"id":"bVUp_mqR6pX0","outputId":"ea27b62f-b200-4d07-9f6c-89daa57f1079","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T15:42:39.230070Z","iopub.execute_input":"2024-11-03T15:42:39.230609Z","iopub.status.idle":"2024-11-03T15:42:39.276036Z","shell.execute_reply.started":"2024-11-03T15:42:39.230560Z","shell.execute_reply":"2024-11-03T15:42:39.274235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"join_tables.sort_values(by=['msno', 'safra'], ascending = False, inplace = True)","metadata":{"_uuid":"92b57346-9a68-4001-a5f7-23de39282d40","_cell_guid":"634cc67b-d608-46bc-b98c-631a4f9aac71","trusted":true,"collapsed":false,"id":"nHMghkGX6pX2","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Treinamento e teste para previsão de 1 mês (M1)\nX_train1, X_test1m, y_train_1m, y_test_1m = train_test_split(x1m, y_1m, test_size=0.3, random_state=42)\ncostumer_churn_prediction(xgc, X_train1, X_test1m, y_train_1m, y_test_1m, \"features\", threshold_plot=True)\n\n# Treinamento e teste para previsão de 2 meses (M2)\nX_train2, X_test2m, y_train_2m, y_test_2m = train_test_split(x2m, y_2m, test_size=0.3, random_state=42)\ncostumer_churn_prediction(xgc, X_train2, X_test2m, y_train_2m, y_test_2m, \"features\", threshold_plot=True)\n\n# Treinamento e teste para previsão de 3 meses (M3)\nX_train3, X_test3m, y_train_3m, y_test_3m = train_test_split(x3m, y_3m, test_size=0.3, random_state=42)\ncostumer_churn_prediction(xgc, X_train3, X_test3m, y_train_3m, y_test_3m, \"features\", threshold_plot=True)\n","metadata":{"_uuid":"f7b19374-d618-44a9-8563-2cdbe0e7b662","_cell_guid":"938aaf9e-2617-49fc-b1f7-7df378cd418b","trusted":true,"collapsed":false,"id":"A6-RKGL96pX2","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Previsão e cálculo do churn em cada janela\nfor period, y_pred, y_test in zip(['1 mês', '2 meses', '3 meses'], \n                                  [y_pred_1m, y_pred_2m, y_pred_3m], \n                                  [y_test_1m, y_test_2m, y_test_3m]):\n    total_churn = (y_pred == 1).sum()\n    print(f'Percentual de clientes com churn em {period}: {total_churn / len(y_pred):.2%}')\n","metadata":{"_uuid":"da07b238-9251-4ebf-95d7-da84caf28c7b","_cell_guid":"cf05069b-fbb9-47fc-8a77-a31908f97b98","trusted":true,"collapsed":false,"id":"OLqofVWJ6pX4","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom math import * # module math\nimport matplotlib.pyplot as plt # visualization\nfrom PIL import Image\nimport seaborn as sns # visualization\nimport itertools\nimport io\nimport plotly.offline as py # visualization\npy.init_notebook_mode(connected=True) # visualization\nimport plotly.graph_objs as go # visualization\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff # visualization\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport statsmodels.api as sm\nfrom yellowbrick.classifier import DiscriminationThreshold\n\n%matplotlib inline","metadata":{"_uuid":"98c6851b-e75a-4308-b33b-7cad7b6ad54d","_cell_guid":"ea716d66-461b-465c-af6f-d3def9e3f497","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def costumer_churn_prediction(algorithm, training_x, testing_x, training_y, testing_y, cf, threshold_plot):\n    #model\n    algorithm.fit(training_x, training_y)\n    predictions = algorithm.predict(testing_x)\n    probabilities = algorithm.predict_proba(testing_x)\n        \n    print('Algorithm:', type(algorithm).__name__)\n    print(\"\\nClassification report:\\n\", classification_report(testing_y, predictions))\n    print(\"Accuracy Score:\", accuracy_score(testing_y, predictions))\n    \n    #confusion matrix\n    conf_matrix = confusion_matrix(testing_y, predictions)\n    #roc_auc_score\n    model_roc_auc = roc_auc_score(testing_y, predictions) \n    print(\"Area under curve:\", model_roc_auc,\"\\n\")\n    \n    fpr, tpr, thresholds = roc_curve(testing_y, probabilities[:,1])\n     \n    #plot confusion matrix\n    trace1 = go.Heatmap(z = conf_matrix,\n                        x = [\"Not churn\", \"Churn\"],\n                        y = [\"Not churn\", \"Churn\"],\n                        showscale = False, colorscale = \"Picnic\",\n                        name = \"Confusion matrix\")\n    \n    #plot roc curve\n    trace2 = go.Scatter(x = fpr, y = tpr,\n                        name = \"Roc: \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'), width = 2))\n    trace3 = go.Scatter(x = [0,1], y = [0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'), width = 2,\n                        dash = 'dot'))\n    \n    if cf in ['coefficients', 'features']:\n        if cf == 'coefficients':\n            coefficients = pd.DataFrame(algorithm.coef_.ravel())\n        elif cf == 'features':\n            coefficients = pd.DataFrame(algorithm.feature_importances_)\n        \n        column_df = pd.DataFrame(training_x.columns.tolist())\n        coef_sumry = (pd.merge(coefficients, column_df, left_index=True, \n                               right_index=True, how=\"left\"))\n        coef_sumry.columns = [\"coefficients\", \"features\"]\n        coef_sumry = coef_sumry.sort_values(by = \"coefficients\", ascending=False)\n        \n        #plot coeffs\n        trace4 = go.Bar(x = coef_sumry[\"features\"], y = coef_sumry[\"coefficients\"], \n                        name = \"coefficients\",\n                        marker = dict(color = coef_sumry[\"coefficients\"],\n                                      colorscale = \"Picnic\",\n                                      line = dict(width = .6, color = \"black\")\n                                     )\n                       )\n        #subplots\n        fig = make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                                subplot_titles=('Confusion matrix',\n                                                'Receiver operating characteristic',\n                                                'Feature importances')\n                           )  \n        fig.append_trace(trace1,1,1)\n        fig.append_trace(trace2,1,2)\n        fig.append_trace(trace3,1,2)\n        fig.append_trace(trace4,2,1)\n        fig['layout'].update(showlegend=False, title=\"Model performance\",\n                             autosize=False, height = 900, width = 800,\n                             plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                             paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                             margin = dict(b = 195))\n        fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n        fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n        fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True, tickfont = dict(size = 10), tickangle = 90))\n        \n    elif cf == 'None':\n        #subplots\n        fig = make_subplots(rows=1, cols=2,\n                            subplot_titles=('Confusion matrix',\n                                            'Receiver operating characteristic')\n                           )\n        fig.append_trace(trace1,1,1)\n        fig.append_trace(trace2,1,2)\n        fig.append_trace(trace3,1,2)\n        fig['layout'].update(showlegend=False, title=\"Model performance\",\n                         autosize=False, height = 500, width = 800,\n                         plot_bgcolor = 'rgba(240,240,240,0.95)',\n                         paper_bgcolor = 'rgba(240,240,240,0.95)',\n                         margin = dict(b = 195))\n        fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n        fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))  \n        \n    py.iplot(fig)\n    \n    if threshold_plot == True: \n        visualizer = DiscriminationThreshold(algorithm)\n        visualizer.fit(training_x,training_y)\n        visualizer.poof()","metadata":{"_uuid":"32f77941-5844-4389-bd06-33f82ca7961e","_cell_guid":"1de71b0e-66f3-43db-be37-25055cf47da2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separar variáveis explicativas e alvo para previsão de churn no próximo mês\n\n# Dividir os dados em conjuntos de treinamento e teste\nX_train1, X_test1m, y_train_1m, y_test_1m = train_test_split(x1m, y_1m, test_size=0.3, random_state=42)\n\n\nfrom xgboost import XGBClassifier\n\nxgc = XGBClassifier(scale_pos_weight=scale_pos_weight,  # Ajuste para classes desbalanceadas\n   use_label_encoder=False, reg_lambda =  0.1, reg_alpha = 0.644, n_estimators = 50,\n                         max_depth = 2, learning_rate = 0.11)\n\ncostumer_churn_prediction(xgc, X_train1, X_test1m, y_train_1m, y_test_1m, \"features\", threshold_plot=True)","metadata":{"_uuid":"639592cc-6d3b-4b41-888e-5a493cc167a6","_cell_guid":"330dac6c-ba70-4da8-88fc-735092605f65","trusted":true,"collapsed":false,"id":"pJnUNWys6pX6","outputId":"3250c4a3-5785-428d-9fb6-c0a5184a47fe","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Percentual de clientes com churn:',(total_churn_1m)/(len(y_pred_1m)))","metadata":{"_uuid":"983167ef-aa1e-4c96-9743-9f9e547dc93a","_cell_guid":"d138e258-c77b-4730-9aa6-ebb25ca3216a","trusted":true,"collapsed":false,"id":"l2NaOuWy6pYV","outputId":"20e1ca6a-940d-4f92-85bc-3514c328dccd","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Conclusão:**\nConseguimos identificar 6% clientes com possível  churn nos proximos três meses.\nEntendendo que X% ficam ativos pós ação, conseguiremos reter X%.\n\nÉ importante passar a lista destes clientes para que a area de negocio entre em contato tentando fortalecer e restabelecer o relacionamento com o objetivo de reduzir o provável churn.\n\nPara finalizar, iremos realizar a analise não supervisionada, agrupar nossos clientes e tentar buscar padrões de churn e comportamento.","metadata":{"_uuid":"aabd3b07-2cd5-493c-96f6-b18d15789af1","_cell_guid":"63a695d2-5a55-4312-9227-1a15392c1555","trusted":true,"collapsed":false,"id":"LTzxrT726pYW","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# 2. Analise de Perfil de clientes","metadata":{"_uuid":"4cc0fb66-e436-4d41-8a38-49089a7dc533","_cell_guid":"9b0db6ba-6dfa-4628-9728-f251743739e5","trusted":true,"collapsed":false,"id":"bMsy0SKt6pYX","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Aqui o objetivo é entender mais sobre o padrão comportamente dos nossos clientes através da analise supervisionada.\nPara tanto, utilizaremos duas técnicas diferentes: de centroides e de hierarquização, para centoide direcionamos ao  K-means e Clusterização hierarquica.\n\nSeguiremos um passo a passo que consiste em:\n\n\n1. Remoção de outliers e PCA\n\n2. Numero de K otimo - Elbow\n\n3. Aplicar a clusterização usando K-means\n\n4. Aplicar a clusterização hierarquica\n\n5. Comparar os resultados - Rand Index e coeifciente de silueta","metadata":{"_uuid":"f2fc83f8-0a0a-4fd1-b2bc-077a24a094e7","_cell_guid":"9f717aca-e12a-49cb-9eb8-fbdf2d119641","trusted":true,"collapsed":false,"id":"aKWAaStx6pYX","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# A.Remoção de outliers e Redução de Dimensionalidade","metadata":{"_uuid":"275528e4-ed20-4c0a-8a41-64afe1fddda4","_cell_guid":"9e06f02a-857c-431a-851d-2ae863300359","trusted":true,"collapsed":false,"id":"twwEzLy86pYX","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Redução de Outliers com Quantis","metadata":{"_uuid":"d8f60682-f775-4cd6-a2e6-13ca9562d14c","_cell_guid":"04451361-9005-44de-bbb3-4194801e53f9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def out_data(df):\n    for col in df.columns:\n        if (((df[col].dtype)=='float64') | ((df[col].dtype)=='int64')):\n            percentiles = df[col].quantile([0.01,0.99]).values\n            df[col][df[col] <= percentiles[0]] = percentiles[0]\n            df[col][df[col] >= percentiles[1]] = percentiles[1]\n        else:\n            df[col]=df[col]\n    return df\n\nfinal_df=out_data(join_tables)","metadata":{"_uuid":"5eb7e7d3-f5e8-493f-8a3a-7734921a20ab","_cell_guid":"c8c54654-bf30-42c4-90eb-96aa683590f6","trusted":true,"collapsed":false,"id":"iqqYOS7S6pYY","outputId":"df93c1ed-304f-4060-b062-7d0ebf7f36fd","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T15:44:12.162843Z","iopub.execute_input":"2024-11-03T15:44:12.163349Z","iopub.status.idle":"2024-11-03T15:44:14.834103Z","shell.execute_reply.started":"2024-11-03T15:44:12.163303Z","shell.execute_reply":"2024-11-03T15:44:14.832997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Devido ao tamanho da base aplicaremos amostragem para categorização. Utilizaremos 30% aleatoriamente definido para a amostra, **amostragem aleatoria simples com reposição**","metadata":{"_uuid":"9d834f12-e562-4654-80ae-d52537353435","_cell_guid":"fe55f63e-d001-4d1c-a03f-e327e88b7698","trusted":true,"collapsed":false,"id":"fLPJo6Tq6pYY","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"sample_1 = final_df[:10000]","metadata":{"_uuid":"c774ea6a-d8cc-4c1f-882d-e1bff1037cf3","_cell_guid":"1a68241a-7726-436b-a7e2-73ca8c2a67ba","trusted":true,"collapsed":false,"id":"EzhS9QUa6pYY","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T15:44:20.890918Z","iopub.execute_input":"2024-11-03T15:44:20.891851Z","iopub.status.idle":"2024-11-03T15:44:20.897488Z","shell.execute_reply.started":"2024-11-03T15:44:20.891798Z","shell.execute_reply":"2024-11-03T15:44:20.896228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# B. Número ideal de k\n\nAs principais metodologias de definição de K são Cotovelo (elbow) e silhueta. Para definir com extatidão qual seguir vamos fazer  um comparação entre elas, com seus prós e contras:\n#### Método do Cotovelo (Elbow)\n#### Descrição:\nO método do cotovelo consiste em calcular a soma das distâncias quadráticas dentro dos clusters (inertia ou WCSS - Within-Cluster Sum of Squares) para diferentes valores de \\( K \\). O valor ideal de \\( K \\) é identificado onde há uma diminuição acentuada na curva, formando um \"cotovelo\".\n\n#### Prós:\n- Simplicidade: Fácil de implementar e interpretar visualmente.\n- Intuitivo: A curva do cotovelo fornece uma maneira visualmente clara de selecionar \\( K \\).\n#### Contras:\n- Subjetivo: A identificação do \"cotovelo\" pode ser ambígua, especialmente em datasets onde a curva é suave.\n- Escalabilidade: Pode ser computacionalmente caro para grandes datasets, pois requer múltiplas execuções do algoritmo de clustering.\n\nA técnica de normalização escolhidade é o minimos máximos\n\n**Devido ao alto processamento computacional seguiremos com o metodo do cotovelo.**","metadata":{"_uuid":"ed79219b-7598-47b8-8590-8032ebdd1f55","_cell_guid":"f97aabe8-2271-4821-bda7-08df171a50d0","trusted":true,"collapsed":false,"id":"1SxRzBOS6pYZ","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Identificar colunas numéricas e categóricas\nnum_cols = sample_1.drop(['msno', 'safra', 'churn_1','is_churn', 'plan_list_price','payment_plan_days','is_auto_renew','bd',\n 'registration_ate_hoje','transaction_date_ate_expire_transactions_m1','transaction_date_ate_expire',\n                          'payment_method_id','payment_plan_days',\n ], axis =1).select_dtypes(include=np.number).columns.tolist()\n\n# Identificar colunas numéricas e categóricas\ncat_cols = sample_1.drop(['msno', 'safra', 'churn_1','is_churn', 'plan_list_price','payment_plan_days','is_auto_renew','bd',\n 'registration_ate_hoje','transaction_date_ate_expire_transactions_m1','transaction_date_ate_expire',\n                          'payment_method_id','payment_plan_days',\n ], axis =1).select_dtypes(include=np.object).columns.tolist()","metadata":{"_uuid":"47665217-36c2-40c8-8317-fa1d34f5c73a","_cell_guid":"84d56945-6d62-4860-90fd-f9adb135f466","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T15:44:25.263339Z","iopub.execute_input":"2024-11-03T15:44:25.263822Z","iopub.status.idle":"2024-11-03T15:44:25.666270Z","shell.execute_reply.started":"2024-11-03T15:44:25.263776Z","shell.execute_reply":"2024-11-03T15:44:25.663613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Iniciando parametros do kmeans\nkmeans_kwargs = {\n\"init\": \"random\",\n\"n_init\": 10,\n\"random_state\": 1,\n}\n\n# Normalizar os dados numéricos\nscaler = MinMaxScaler()\nscaler = scaler.fit(sample_1[num_cols])\nsample_1[num_cols] = scaler.transform(sample_1[num_cols])\n#print(scaled)\n\n\n#create list to hold SSE values for each k\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(sample_1[num_cols])\n    sse.append(kmeans.inertia_)\n\n#visualize results\nplt.plot(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Number of Clusters nobs \")\nplt.ylabel(\"SSE\")\nplt.show()","metadata":{"_uuid":"e3f709d9-8910-41aa-942d-c3bc70bb346f","_cell_guid":"7f0fb630-e8c1-475a-87c8-c057740fecd1","trusted":true,"collapsed":false,"id":"oceUmR3H6pYa","outputId":"e63e88e0-49ea-4ec0-9d05-bf62b8b59aa2","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# C. Redução de dimensionalidade (PCA)","metadata":{"_uuid":"7c76955b-5228-494c-ab2e-3a5be5886914","_cell_guid":"ee0eb44c-91da-4d3c-888b-21d0f8d4273e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\npca = PCA(2)\ndata_pca = pca.fit_transform(sample_1[num_cols])","metadata":{"_uuid":"c46aa6f9-e802-4419-a23e-8e7f9f944c4f","_cell_guid":"946752b1-90f9-4a34-922e-5109038d83cf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-03T15:44:35.453139Z","iopub.execute_input":"2024-11-03T15:44:35.453649Z","iopub.status.idle":"2024-11-03T15:44:35.493809Z","shell.execute_reply.started":"2024-11-03T15:44:35.453601Z","shell.execute_reply":"2024-11-03T15:44:35.491969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.scatter(data_pca[:, 0], data_pca[:, 1],\n            c=sample_1.is_churn, edgecolor='none', alpha=0.5)\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.colorbar();","metadata":{"_uuid":"686da8be-d1dd-4a15-b245-4e16636a04a2","_cell_guid":"b09d165e-ab95-41f5-8559-76eb8430c1e0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# D. Clusterização\nPara o agrupamento utilizamos duas técnicas: k-means e GMM","metadata":{"_uuid":"274ac939-e1e5-464a-9ba7-3be8b8823ee6","_cell_guid":"87267960-1abc-4199-9784-8b93280921d8","trusted":true,"collapsed":false,"id":"eTKQyLgz6pYa","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### Clusterização usando K-means","metadata":{"_uuid":"2487a899-7850-4448-9f1c-f56511f260ab","_cell_guid":"b006b541-cd20-4e33-afff-2ba8b783eba0","trusted":true,"collapsed":false,"id":"AQhe6LOX6pYa","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Aplicando K-Means\nkmeans = KMeans(n_clusters=4, random_state=42)\nkmeans_labels = kmeans.fit_predict(data_pca)\nkmeans_silhouette = silhouette_score(data_pca, kmeans_labels)\n\n# salvando na base\nsample_1['kmeans_cluster'] = kmeans_labels\n\n\n# Métrica de qualidade\nprint(f'Coeficiente de Silhueta - KMeans: {kmeans_silhouette:.4f}')","metadata":{"_uuid":"ba019390-71f8-4f5e-8895-abf06a5a5af0","_cell_guid":"d3c568ae-5d01-4bec-bdab-6d82f853c638","trusted":true,"collapsed":false,"id":"LGFdsViN6pYb","outputId":"e7b1ea24-23d1-4c00-87b2-b6668ce70997","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for inverse transformation\nsample_1[num_cols] = scaler.inverse_transform(sample_1[num_cols])\nsample_1.groupby('kmeans_cluster').describe()","metadata":{"_uuid":"6d462be9-11b3-4b2e-8157-751a602b6138","_cell_guid":"73ab7911-2954-45bf-bf31-3818abb21c88","trusted":true,"collapsed":false,"id":"pwa3Evmh6pYd","outputId":"caf1a54f-962b-4258-caa7-d495ecdd1174","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Clusterização usando GMM\n\nPara garantir que os dados estejam mais próximos de uma distribuição normal, você pode aplicar uma transformação logarítmica ou a transformação Box-Cox (que é mais flexível) antes da normalização. A seguir, o código atualizado com a transformação Box-Cox (caso os dados sejam estritamente positivos) antes da normalização.\n\n### Explicações:\n1. **PowerTransformer (Box-Cox)**: Essa transformação aproxima a normalidade para variáveis positivas, ajustando distribuições assimétricas ou com caudas longas. Use `method='box-cox'` para a transformação Box-Cox.\n  - Nota: O Box-Cox requer que os dados sejam **estritamente positivos**.\n  - Caso você tenha variáveis que incluem valores zero ou negativos, pode usar `method='yeo-johnson'`, que também é uma transformação de potência, mas não tem essa restrição.\n2. **StandardScaler**: Normaliza as variáveis numéricas transformadas para média 0 e desvio padrão 1.\n3. **OneHotEncoder**: Codifica as variáveis categóricas em formato binário (dummy variables).\n### Resultados:\nEsse pipeline agora trata adequadamente os dados numéricos, aplicando uma transformação que aproxima uma distribuição normal e em seguida normaliza os dados para que o **GMM** funcione melhor, especialmente em situações onde as variáveis não são originalmente gaussianas.\nSe você tiver dúvidas sobre o uso do Box-Cox ou a necessidade de outro tipo de transformação, é só avisar!","metadata":{"_uuid":"4312281a-3070-41b5-b01f-8f6dbc4e3c14","_cell_guid":"40798177-e0b1-4d3c-b805-6f82604144ff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"x.describe()","metadata":{"_uuid":"2a0a62fc-29a1-4b36-b70e-2b078990536d","_cell_guid":"30fdb289-61e6-4b69-84cf-be88d4f83d11","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_1.describe()","metadata":{"_uuid":"9e7122c0-4700-4246-86f5-dfa8edd5a9c6","_cell_guid":"ebda9b07-a003-47a0-80c9-45a8603d1d0e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.mixture import GaussianMixture\n\n# Criar transformações\ntransformacoes = ColumnTransformer(transformers=[\n   ('num', Pipeline(steps=[\n       #('boxcox', PowerTransformer(method='box-cox')),  # Transformação Box-Cox para aproximar normalidade\n       ('scaler', StandardScaler())  # Normalização\n   ]), num_cols),\n   ('cat', OneHotEncoder(), cat_cols)\n])\n# Criar pipeline com pré-processamento e GMM\npipeline = Pipeline(steps=[\n   ('pre_processamento', transformacoes),\n   ('gmm', GaussianMixture(n_components=3, random_state=42))\n])\n# Ajustar modelo\npipeline.fit(sample_1)\n# Prever clusters\nclusters = pipeline['gmm'].predict(pipeline['pre_processamento'].transform(sample_1))\nprint(\"Clusters:\", clusters)","metadata":{"_uuid":"e8f9fac0-e8df-421b-aa9b-621a8f89f652","_cell_guid":"8cc85ffa-9c0b-462e-b712-d4410b777a33","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Conclusão:**\nApós a escolha de 4 grupos através da distãncia intra clusters de elbow,clusterizamos com o algoritmo kmeans e abordagem de distancia euclidiana. Os grupos apresentaram distinção entre quem deu churn ou não, no mes target e outros. Outro  insight relevante foi a distinção de audição entre os grupos, e uma relação observada entre churn e a propria audição dos grupos.","metadata":{"_uuid":"87f83623-93a3-4525-8004-bd5cd62bf6ef","_cell_guid":"1db943e5-d699-4274-9b4f-d16fd1de65fa","trusted":true,"collapsed":false,"id":"OWuGXcnv6pYe","jupyter":{"outputs_hidden":false}}}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feature Selection\n\nÉ interessante separar o que realmente influencia ou não no churn, para tanto, devido a termos uma base mixada (variaveis categoricas e numericas) utilizaremos uma abordagem baseada em ensemble.\n\nLightGBM é extremamente eficiente para data frames grandes.\nComo o Random Forest, ele também fornece importâncias de variáveis, mas é otimizado para desempenho em datasets grandes e com alto número de features.\nEle pode lidar bem com variáveis categóricas, e geralmente é mais rápido que Random Forest devido à sua implementação baseada em histogramas. Além disso, atende bem problemas desbalanceados como  o nosso,","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Definindo o modelo LightGBM para feature selection\nlgb_model = lgb.LGBMClassifier(random_state=42, n_estimators=500)\n\n# Treinando o modelo com todas as features\nlgb_model.fit(X_train, y_train)\n\n# Extraindo as importâncias das features\nfeature_importances = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': lgb_model.feature_importances_\n})\n\n# Ordenando por importância e selecionando as principais variáveis\nfeature_importances = feature_importances.sort_values(by='importance', ascending=False)\n\n# Definindo um número de features a serem mantidas\ntop_k = 20\ntop_features = feature_importances.head(top_k)['feature'].values\n\n# Selecionando as features mais importantes no conjunto de dados\nX_train_selected = X_train[top_features]\nX_val_selected = X_val[top_features]\nX_test_selected = X_test[top_features]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verificação das dimensões após a seleção\nprint(\"Dimensões do conjunto de treino com features selecionadas:\", X_train_selected.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Dimensões do conjunto de validação com features selecionadas:\", X_val_selected.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Dimensões do conjunto de teste com features selecionadas:\", X_test_selected.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}